{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f270f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:02:32.525256Z",
     "iopub.status.busy": "2024-04-22T10:02:32.524787Z",
     "iopub.status.idle": "2024-04-22T10:02:47.925684Z",
     "shell.execute_reply": "2024-04-22T10:02:47.923126Z"
    },
    "papermill": {
     "duration": 15.414109,
     "end_time": "2024-04-22T10:02:47.929001",
     "exception": false,
     "start_time": "2024-04-22T10:02:32.514892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reportlab\r\n",
      "  Downloading reportlab-4.2.0-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from reportlab) (9.5.0)\r\n",
      "Collecting chardet (from reportlab)\r\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Downloading reportlab-4.2.0-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: chardet, reportlab\r\n",
      "Successfully installed chardet-5.2.0 reportlab-4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "# Installing reportlab which is used for creating PDFs\n",
    "! pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a78b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:02:47.946376Z",
     "iopub.status.busy": "2024-04-22T10:02:47.944886Z",
     "iopub.status.idle": "2024-04-22T10:04:16.657183Z",
     "shell.execute_reply": "2024-04-22T10:04:16.654372Z"
    },
    "papermill": {
     "duration": 88.724314,
     "end_time": "2024-04-22T10:04:16.660697",
     "exception": false,
     "start_time": "2024-04-22T10:02:47.936383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-22 10:03:06--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.70, 13.227.219.59, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 4503593528 (4.2G) [application/octet-stream]\r\n",
      "Saving to: 'cc.en.300.bin.gz'\r\n",
      "\r\n",
      "cc.en.300.bin.gz    100%[===================>]   4.19G   317MB/s    in 13s     \r\n",
      "\r\n",
      "2024-04-22 10:03:19 (326 MB/s) - 'cc.en.300.bin.gz' saved [4503593528/4503593528]\r\n",
      "\r\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data/...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data/...\n",
      "Archive:  /kaggle/working/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /kaggle/working/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/wordnet/adj.exc  \r\n",
      "Archive:  /kaggle/working/nltk_data/corpora/omw-1.4.zip\r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/\r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/wn-data-fin.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/wn-data-heb.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/wn-data-slv.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/wn-data-ita.tab  \r\n",
      " extracting: /kaggle/working/nltk_data/corpora/omw-1.4/ita/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nno.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nob.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/als/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/wn-data-als.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/wn-data-pol.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/\r\n",
      " extracting: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/wn-data-hrv.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/citation.bib  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/wn-data-ita.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/wn-data-nld.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/citation.bib  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/wn-data-ron.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/wn-data-arb.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/wn-data-isl.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/wn-data-swe.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/por/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/wn-data-por.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/wn-data-cmn.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/citation.bib  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/README  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/wn-data-jpn.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/wn-data-dan.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-slk.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-lit.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/wn-data-bul.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-eus.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-cat.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-glg.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-spa.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/wn-data-ell.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-zsm.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-ind.tab  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/README  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/wn-data-fra.tab  \r\n",
      "   creating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/\r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/LICENSE  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/citation.bib  \r\n",
      "  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/wn-data-tha.tab  \r\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from gensim.models.fasttext import FastText # build and train Fast Text model\n",
    "from gensim.models import Word2Vec # to Load the saved model\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "# Downloading the pre-trained model from a website\n",
    "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "! gunzip \"cc.en.300.bin.gz\"\n",
    "nltk.download('wordnet', \"/kaggle/working/nltk_data/\")\n",
    "nltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n",
    "! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n",
    "! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\n",
    "# Adding this path to nltk so it can observe the files of the packages in it\n",
    "nltk.data.path.append(\"/kaggle/working/nltk_data/\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download English stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78495b9",
   "metadata": {
    "papermill": {
     "duration": 0.012073,
     "end_time": "2024-04-22T10:04:16.686548",
     "exception": false,
     "start_time": "2024-04-22T10:04:16.674475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b872d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:16.719529Z",
     "iopub.status.busy": "2024-04-22T10:04:16.718409Z",
     "iopub.status.idle": "2024-04-22T10:04:23.786534Z",
     "shell.execute_reply": "2024-04-22T10:04:23.784428Z"
    },
    "papermill": {
     "duration": 7.086459,
     "end_time": "2024-04-22T10:04:23.789901",
     "exception": false,
     "start_time": "2024-04-22T10:04:16.703442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all columns\n",
      "['user_id', 'business_id', 'text', 'date', 'compliment_count']\n"
     ]
    }
   ],
   "source": [
    "# Handling the Yelp Dataset\n",
    "data_file_path = \"/kaggle/input/yelp_academic_dataset_tip.json\"\n",
    "data_file_name = \"yelp_academic_dataset_tip.json\"\n",
    "yelp_datafile = pd.read_json(data_file_path, lines=True)\n",
    "print('List of all columns')\n",
    "print(list(yelp_datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11beb163",
   "metadata": {
    "papermill": {
     "duration": 0.012388,
     "end_time": "2024-04-22T10:04:23.814883",
     "exception": false,
     "start_time": "2024-04-22T10:04:23.802495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Taking subset for Gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88df9c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:23.846094Z",
     "iopub.status.busy": "2024-04-22T10:04:23.845634Z",
     "iopub.status.idle": "2024-04-22T10:04:24.032515Z",
     "shell.execute_reply": "2024-04-22T10:04:24.031635Z"
    },
    "papermill": {
     "duration": 0.205053,
     "end_time": "2024-04-22T10:04:24.035110",
     "exception": false,
     "start_time": "2024-04-22T10:04:23.830057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples of Sentences\n",
      " [['Avengers time with the ladies.', 'They have lots of good deserts and tasty cuban sandwiches', \"It's open even when you think it isn't\", 'Very decent fried chicken', 'Appetizers.. platter special for lunch', 'Chili Cup + Single Cheeseburger with onion, pickle, and relish + Vanilla Coca-Cola...so far.', \"Saturday, Dec 7th 2013, ride Patco's Silver Sleigh w/ Santa & his elves on a decorated train into Center City. Trains leave from Lindenwold at 10am, 11:15am, & 12:30pm, and make all stops. Great for kids!\", 'This is probably the best place in the cool Springs area to watch a game and eat', 'Tacos', 'Starbucks substitute in boring downtown Tampa. Ugh. Never again!']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subset data for gensim fastText model\n",
    "all_sentences = list(yelp_datafile['text']) # select \"text\" column only\n",
    "part_of_sentences = all_sentences[0:3000] # select the first 3000 sample lines\n",
    "print(\"\\nSamples of Sentences\\n [{}]\".format(part_of_sentences[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9954a",
   "metadata": {
    "papermill": {
     "duration": 0.013357,
     "end_time": "2024-04-22T10:04:24.061412",
     "exception": false,
     "start_time": "2024-04-22T10:04:24.048055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing the subset we took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4123bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:24.090636Z",
     "iopub.status.busy": "2024-04-22T10:04:24.090109Z",
     "iopub.status.idle": "2024-04-22T10:04:26.585893Z",
     "shell.execute_reply": "2024-04-22T10:04:26.583856Z"
    },
    "papermill": {
     "duration": 2.514536,
     "end_time": "2024-04-22T10:04:26.589111",
     "exception": false,
     "start_time": "2024-04-22T10:04:24.074575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def process_text(document):\n",
    "    \n",
    "    document = re.sub(r'[^a-zA-Z0-9\\s]', '', document)# Remove non-alphanumeric characters\n",
    "    \n",
    "    document = re.sub(r'\\w\\d\\w', '', document)# Removing words that have numbers in them\n",
    "    \n",
    "    document = re.sub(r'[0-9]+', '', document) # Removing digits\n",
    "    \n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I) # Remove extra white space from text\n",
    "\n",
    "    document = re.sub(r'\\W', ' ', str(document)) # Remove all the special characters from text\n",
    "\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document) # Remove all single characters from text\n",
    "    \n",
    "    document = document.lower() # Converting to Lowercase\n",
    "    \n",
    "    # Word tokenization \n",
    "    tokens = document.split()\n",
    "    \n",
    "    # Applying lemmatization\n",
    "    lemma_txt = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Removing stopping words\n",
    "    lemma_no_stop_txt = [word for word in lemma_txt if word not in english_stopwords]\n",
    "    \n",
    "    # Drop words less than 3 characters\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    \n",
    "    # Getting unique words\n",
    "    indices = np.unique(tokens, return_index=True)[1]\n",
    "    cleaned_unique = np.array(tokens)[indices].tolist()\n",
    "    \n",
    "    return cleaned_unique\n",
    "cleaned_reviews = [ process_text(document) for document in part_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32228e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:26.623092Z",
     "iopub.status.busy": "2024-04-22T10:04:26.622561Z",
     "iopub.status.idle": "2024-04-22T10:04:26.630234Z",
     "shell.execute_reply": "2024-04-22T10:04:26.628474Z"
    },
    "papermill": {
     "duration": 0.03078,
     "end_time": "2024-04-22T10:04:26.633881",
     "exception": false,
     "start_time": "2024-04-22T10:04:26.603101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: ['avengers', 'ladies', 'time', 'with']\n",
      "Review 2: ['cuban', 'deserts', 'good', 'have', 'lots', 'sandwiches', 'tasty', 'they']\n",
      "Review 3: ['even', 'isnt', 'open', 'think', 'when']\n",
      "Review 4: ['chicken', 'decent', 'fried', 'very']\n",
      "Review 5: ['appetizers', 'lunch', 'platter', 'special']\n",
      "Review 6: ['cheeseburger', 'chili', 'cocacolaso', 'onion', 'pickle', 'relish', 'single', 'vanilla', 'with']\n",
      "Review 7: ['center', 'city', 'decorated', 'elves', 'from', 'great', 'into', 'kids', 'leave', 'lindenwold', 'make', 'patcos', 'ride', 'santa', 'saturday', 'silver', 'sleigh', 'stops', 'train', 'trains']\n",
      "Review 8: ['area', 'best', 'cool', 'game', 'place', 'probably', 'springs', 'this', 'watch']\n",
      "Review 9: ['tacos']\n",
      "Review 10: ['again', 'boring', 'downtown', 'never', 'starbucks', 'substitute', 'tampa']\n",
      "Review 11: ['order', 'soup', 'tortilla']\n",
      "Review 12: ['back', 'coming', 'definitely', 'good', 'very', 'will']\n",
      "Review 13: ['hotlight', 'must', 'stop']\n",
      "Review 14: ['lets', 'yankees']\n",
      "Review 15: ['basically', 'food', 'more', 'rallys', 'same']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 15 processed reviews\n",
    "for idx,document in enumerate(cleaned_reviews[0:15]):\n",
    "    print(f\"Review {idx+1}: {document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc859b32",
   "metadata": {
    "papermill": {
     "duration": 0.012826,
     "end_time": "2024-04-22T10:04:26.659829",
     "exception": false,
     "start_time": "2024-04-22T10:04:26.647003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train our FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2695ec8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:26.687508Z",
     "iopub.status.busy": "2024-04-22T10:04:26.687060Z",
     "iopub.status.idle": "2024-04-22T10:04:26.696662Z",
     "shell.execute_reply": "2024-04-22T10:04:26.694420Z"
    },
    "papermill": {
     "duration": 0.027268,
     "end_time": "2024-04-22T10:04:26.700130",
     "exception": false,
     "start_time": "2024-04-22T10:04:26.672862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_Fasttext(sentences,embedding_size,window_size,min_word,down_sampling,epochs,Save_model_filename):\n",
    "    fast_Text_model = FastText(sentences,\n",
    "    vector_size=embedding_size, # Dimensionality of the word vectors. ,\n",
    "    window=window_size,\n",
    "    min_count=min_word, # The model ignores all words with total frequency lower than this.\n",
    "    sample=down_sampling, # threshold which higher-frequency words are randomly down sampled\n",
    "    workers = 4, # Num threads to train the model (faster training with multicore comp.)\n",
    "    sg=1, # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "    epochs=epochs) # Number of iterations (epochs) over the corpus\n",
    "\n",
    "    fast_Text_model.save(Save_model_filename) # Save fastText gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f133e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:04:26.731657Z",
     "iopub.status.busy": "2024-04-22T10:04:26.731062Z",
     "iopub.status.idle": "2024-04-22T10:05:14.391052Z",
     "shell.execute_reply": "2024-04-22T10:05:14.388853Z"
    },
    "papermill": {
     "duration": 47.679977,
     "end_time": "2024-04-22T10:05:14.394174",
     "exception": false,
     "start_time": "2024-04-22T10:04:26.714197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selected values for Training parameters\n",
    "embedding_size = 500\n",
    "window_size = 4\n",
    "min_word = 4\n",
    "down_sampling = 1e-2\n",
    "epochs=200\n",
    "\n",
    "train_Fasttext(cleaned_reviews,embedding_size,window_size,min_word,down_sampling,epochs,\"Custom_FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8c6a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:05:14.471965Z",
     "iopub.status.busy": "2024-04-22T10:05:14.471402Z",
     "iopub.status.idle": "2024-04-22T10:05:16.216121Z",
     "shell.execute_reply": "2024-04-22T10:05:16.213778Z"
    },
    "papermill": {
     "duration": 1.791816,
     "end_time": "2024-04-22T10:05:16.219765",
     "exception": false,
     "start_time": "2024-04-22T10:05:14.427949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"/kaggle/working/Custom_FastText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272b973",
   "metadata": {
    "papermill": {
     "duration": 0.01345,
     "end_time": "2024-04-22T10:05:16.247841",
     "exception": false,
     "start_time": "2024-04-22T10:05:16.234391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "438489dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:05:16.279641Z",
     "iopub.status.busy": "2024-04-22T10:05:16.278022Z",
     "iopub.status.idle": "2024-04-22T10:07:05.674169Z",
     "shell.execute_reply": "2024-04-22T10:07:05.670957Z"
    },
    "papermill": {
     "duration": 109.415678,
     "end_time": "2024-04-22T10:07:05.677527",
     "exception": false,
     "start_time": "2024-04-22T10:05:16.261849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pretrained fastText word embeddings\n",
    "pretrained_fastText_en = load_facebook_model('/kaggle/working/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bf58f",
   "metadata": {
    "papermill": {
     "duration": 0.014286,
     "end_time": "2024-04-22T10:07:05.706945",
     "exception": false,
     "start_time": "2024-04-22T10:07:05.692659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Collect word in our model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce3a481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:05.735937Z",
     "iopub.status.busy": "2024-04-22T10:07:05.735541Z",
     "iopub.status.idle": "2024-04-22T10:07:05.741283Z",
     "shell.execute_reply": "2024-04-22T10:07:05.739612Z"
    },
    "papermill": {
     "duration": 0.02372,
     "end_time": "2024-04-22T10:07:05.743344",
     "exception": false,
     "start_time": "2024-04-22T10:07:05.719624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = list(fast_Text_model.wv.key_to_index)  # Collect words from the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c90e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:05.771368Z",
     "iopub.status.busy": "2024-04-22T10:07:05.770821Z",
     "iopub.status.idle": "2024-04-22T10:07:05.779268Z",
     "shell.execute_reply": "2024-04-22T10:07:05.777488Z"
    },
    "papermill": {
     "duration": 0.026554,
     "end_time": "2024-04-22T10:07:05.782500",
     "exception": false,
     "start_time": "2024-04-22T10:07:05.755946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'good', 'with', 'food', 'this', 'they', 'place', 'service', 'best', 'here', 'have', 'very', 'love', 'your', 'time', 'that', 'nice', 'amazing', 'like', 'delicious', 'friendly', 'dont', 'awesome', 'just', 'lunch', 'free', 'their', 'will', 'come', 'menu', 'chicken', 'always', 'only', 'there', 'order', 'from', 'pizza', 'staff', 'back', 'coffee', 'ever', 'breakfast', 'more', 'fresh', 'really', 'beer', 'make', 'open', 'wait', 'some', 'little', 'happy', 'excellent', 'today', 'what', 'after', 'were', 'when', 'everything', 'than', 'hour', 'salad', 'worth', 'sure', 'much', 'price', 'selection', 'pretty', 'people', 'right', 'closed', 'well', 'better', 'atmosphere', 'parking', 'sushi', 'cheese', 'fried', 'about', 'yummy', 'been', 'check', 'soup', 'even', 'night', 'down', 'want', 'early', 'location', 'favorite', 'shrimp', 'specials', 'restaurant', 'work', 'take', 'must', 'dinner', 'cant', 'drink', 'before', 'wings', 'perfect', 'other', 'area', 'special', 'drinks', 'wine', 'around', 'burgers', 'sweet', 'call', 'cool', 'youre', 'fries', 'them', 'never', 'sandwich', 'still', 'first', 'lots', 'every', 'quick', 'need', 'store', 'again', 'spot', 'bring', 'prices', 'town', 'tasty', 'busy', 'customer', 'music', 'getting', 'minutes', 'know', 'slow', 'also', 'until', 'made', 'clean', 'business', 'small', 'because', 'sunday', 'tacos', 'local', 'experience', 'burger', 'next', 'nothing', 'kids', 'which', 'super', 'last', 'rice', 'half', 'brunch', 'chocolate', 'patio', 'fish', 'hours', 'yelp', 'side', 'live', 'looks', 'outside', 'sauce', 'long', 'ordered', 'week', 'green', 'thats', 'spicy', 'options', 'over', 'anything', 'monday', 'stop', 'would', 'family', 'huge', 'coming', 'same', 'roll', 'going', 'park', 'close', 'beautiful', 'didnt', 'where', 'cold', 'rolls', 'steak', 'server', 'while', 'soon', 'full', 'available', 'white', 'years', 'plus', 'look', 'morning', 'chips', 'taste', 'tonight', 'came', 'bread', 'items', 'most', 'cream', 'though', 'beef', 'home', 'fast', 'weekend', 'sundays', 'room', 'shop', 'sign', 'street', 'oysters', 'burrito', 'inside', 'find', 'large', 'theyre', 'quality', 'birthday', 'theres', 'cheap', 'could', 'recommend', 'gets', 'line', 'miss', 'else', 'seafood', 'then', 'plenty', 'many', 'late', 'game', 'pork', 'through', 'makes', 'table', 'water', 'tuesday', 'cash', 'during', 'enough', 'later', 'another', 'seating', 'hard', 'fantastic', 'visit', 'potato', 'dessert', 'went', 'waiting', 'watch', 'thanks', 'helpful', 'house', 'thing', 'regular', 'eating', 'meal', 'sandwiches', 'authentic', 'phone', 'think', 'said', 'away', 'italian', 'seems', 'flavors', 'cake', 'owner', 'show', 'taco', 'wonderful', 'saturday', 'wifi', 'owners', 'buffet', 'looking', 'walk', 'baked', 'doing', 'keep', 'hands', 'disappointed', 'feel', 'spring', 'under', 'drive', 'manager', 'called', 'style', 'crazy', 'wont', 'expensive', 'told', 'meat', 'onion', 'maybe', 'cute', 'party', 'summer', 'enjoy', 'beans', 'door', 'these', 'highly', 'friday', 'help', 'trying', 'card', 'start', 'dining', 'wrong', 'terrible', 'times', 'both', 'dumplings', 'less', 'mondays', 'wednesday', 'avoid', 'wasnt', 'money', 'crispy', 'ahead', 'worst', 'healthy', 'city', 'into', 'kitchen', 'wish', 'meeting', 'choice', 'give', 'tastes', 'should', 'extremely', 'bacon', 'review', 'corn', 'reservation', 'frozen', 'pool', 'starbucks', 'downtown', 'probably', 'okay', 'prepared', 'chili', 'decent', 'cuban', 'grilled', 'person', 'longer', 'found', 'done', 'homemade', 'eggs', 'something', 'beers', 'charge', 'black', 'especially', 'milk', 'flavor', 'veggie', 'crowd', 'market', 'variety', 'having', 'west', 'used', 'event', 'yourself', 'dogs', 'lobster', 'deal', 'turkey', 'serve', 'pepper', 'duck', 'short', 'cards', 'rude', 'curry', 'thursday', 'delivery', 'please', 'leave', 'each', 'horrible', 'does', 'taking', 'crab', 'poor', 'youll', 'between', 'bomb', 'salty', 'cocktails', 'thai', 'portion', 'employees', 'extra', 'loud', 'everyone', 'working', 'waiter', 'outdoor', 'tampa', 'ready', 'friends', 'needs', 'ladies', 'instead', 'post', 'french', 'appetizer', 'byob', 'finally', 'waitress', 'margaritas', 'things', 'lovely', 'weekends', 'crowded', 'orders', 'limited', 'being', 'appointment', 'blueberry', 'walking', 'credit', 'yogurt', 'philly', 'across', 'tour', 'doesnt', 'outstanding', 'lines', 'days', 'incredible', 'cafe', 'three', 'light', 'slice', 'holiday', 'average', 'delish', 'whole', 'offer', 'nashville', 'portions', 'value', 'world', 'took', 'dirty', 'such', 'pickles', 'comfortable', 'glass', 'chinese', 'shot', 'mango', 'definitely', 'plate', 'save', 'office', 'high', 'month', 'brew', 'floor', 'year', 'vegan', 'front', 'since', 'salsa', 'afternoon', 'interesting', 'strawberry', 'catch', 'usually', 'lets', 'option', 'takes', 'checked', 'bathroom', 'tomato', 'stuff', 'different', 'forget', 'says', 'actually', 'sell', 'nights', 'empty', 'expect', 'once', 'absolutely', 'move', 'cooked', 'building', 'choose', 'ribs', 'isnt', 'roasted', 'veggies', 'waited', 'salads', 'treat', 'honey', 'tried', 'double', 'salmon', 'thank', 'unique', 'smells', 'online', 'real', 'comes', 'margarita', 'pancakes', 'bloody', 'fabulous', 'anywhere', 'foods', 'tasting', 'together', 'classic', 'changes', 'espresso', 'thursdays', 'iced', 'mini', 'fair', 'dish', 'sales', 'flavorful', 'least', 'kind', 'located', 'yeah', 'garden', 'nail', 'peppers', 'owned', 'waffles', 'heaven', 'impressed', 'tofu', 'skip', 'serving', 'bottles', 'sliders', 'draft', 'almost', 'ambiance', 'diner', 'toast', 'website', 'filet', 'sides', 'chai', 'clear', 'warm', 'easy', 'overpriced', 'issues', 'generous', 'checkin', 'bartender', 'burritos', 'affordable', 'apple', 'however', 'bathrooms', 'appetizers', 'reservations', 'club', 'ginger', 'type', 'class', 'able', 'opinion', 'grab', 'without', 'quite', 'daily', 'might', 'recommended', 'places', 'roast', 'pasta', 'tell', 'space', 'stars', 'gift', 'entree', 'cozy', 'section', 'croissants', 'couldnt', 'awful', 'milkshakes', 'bisque', 'chef', 'meats', 'hole', 'wall', 'season', 'consistent', 'advance', 'pick', 'cheesesteak', 'wouldnt', 'served', 'along', 'plain', 'chance', 'latte', 'eaten', 'hummus', 'packed', 'fill', 'dishes', 'spinach', 'heard', 'believe', 'stay', 'hoagies', 'evening', 'feeling', 'rush', 'state', 'irish', 'bites', 'list', 'college', 'blue', 'using', 'tuesdays', 'exactly', 'self', 'several', 'mary', 'juice', 'arrive', 'locally', 'baby', 'football', 'beat', 'catfish', 'sample', 'typical', 'vanilla', 'killer', 'date', 'center', 'pricey', 'wanted', 'chicago', 'massage', 'mimosas', 'board', 'snack', 'cleaning', 'already', 'school', 'dude', 'sitting', 'asked', 'rock', 'alcohol', 'nasty', 'truffle', 'months', 'tuna', 'mexican', 'wednesdays', 'bakery', 'view', 'cakes', 'gotta', 'shes', 'life', 'pleasant', 'waste', 'window', 'dancing', 'guys', 'alternative', 'closing', 'near', 'counter', 'currently', 'girl', 'theyll', 'christmas', 'brand', 'care', 'lobby', 'offering', 'sucks', 'chip', 'seen', 'games', 'four', 'opening', 'garlic', 'matter', 'fall', 'sangria', 'reasonable', 'shopping', 'wrap', 'orange', 'size', 'second', 'vegetarian', 'stopped', 'round', 'youve', 'views', 'coconut', 'stick', 'seriously', 'checking', 'behind', 'moved', 'wife', 'bowl', 'organized', 'venue', 'accommodating', 'nachos', 'gelato', 'normally', 'spaghetti', 'totally', 'machine', 'popcorn', 'unfortunately', 'mins', 'station', 'rather', 'forward', 'indy', 'banana', 'sports', 'prime', 'incredibly', 'minute', 'cups', 'bartenders', 'pete', 'priced', 'reasonably', 'reviews', 'cook', 'lettuce', 'idea', 'dark', 'lost', 'quiet', 'number', 'groceries', 'hope', 'seats', 'single', 'note', 'relish', 'arrived', 'pass', 'tasted', 'running', 'cashier', 'butter', 'platter', 'protein', 'liked', 'cheeses', 'rooms', 'locals', 'smooth', 'simple', 'filling', 'neighborhood', 'hurry', 'falafel', 'words', 'louis', 'blackberry', 'bucks', 'products', 'cajun', 'split', 'playing', 'play', 'vodka', 'bottle', 'true', 'part', 'friend', 'smelled', 'weeks', 'warning', 'unless', 'head', 'stout', 'asian', 'offered', 'guacamole', 'lasagna', 'velvet', 'bottomless', 'seasoned', 'foot', 'dollar', 'movie', 'creme', 'taken', 'purchase', 'return', 'changed', 'charbroiled', 'sugar', 'discount', 'boba', 'lucky', 'menus', 'beware', 'caramel', 'kinda', 'trust', 'tables', 'mind', 'started', 'entire', 'valet', 'donut', 'courtyard', 'left', 'forever', 'coke', 'clothes', 'sister', 'deli', 'seemed', 'craft', 'plan', 'ends', 'enjoyed', 'hair', 'waffle', 'tomorrow', 'mmmm', 'king', 'book', 'bite', 'stale', 'guess', 'gone', 'pack', 'rings', 'name', 'address', 'tender', 'heres', 'gave', 'hate', 'karaoke', 'bean', 'cocktail', 'saturdays', 'meatballs', 'knows', 'sale', 'fruit', 'mediocre', 'soda', 'chipotle', 'insane', 'shoe', 'anyone', 'biscuits', 'possibly', 'bank', 'ownership', 'attentive', 'raining', 'seat', 'strange', 'mark', 'solid', 'face', 'gluten', 'sunny', 'hand', 'cider', 'buffalo', 'american', 'fine', 'services', 'above', 'pickup', 'coupon', 'bowling', 'simply', 'cost', 'write', 'beignets', 'secret']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414e3d4",
   "metadata": {
    "papermill": {
     "duration": 0.013239,
     "end_time": "2024-04-22T10:07:05.808715",
     "exception": false,
     "start_time": "2024-04-22T10:07:05.795476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Printing some random words results to see models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd749a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:05.843590Z",
     "iopub.status.busy": "2024-04-22T10:07:05.842750Z",
     "iopub.status.idle": "2024-04-22T10:07:14.695912Z",
     "shell.execute_reply": "2024-04-22T10:07:14.695110Z"
    },
    "papermill": {
     "duration": 8.873433,
     "end_time": "2024-04-22T10:07:14.698648",
     "exception": false,
     "start_time": "2024-04-22T10:07:05.825215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing word: used\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "closed: 0.4776\n",
      "started: 0.4738\n",
      "seemed: 0.4526\n",
      "tried: 0.4423\n",
      "ownership: 0.4421\n",
      "waited: 0.4216\n",
      "seasoned: 0.4210\n",
      "waste: 0.4143\n",
      "turkey: 0.4119\n",
      "bucks: 0.4117\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "meatballs: 0.0317\n",
      "flavors: 0.0260\n",
      "amazing: 0.0234\n",
      "half: 0.0191\n",
      "always: 0.0164\n",
      "wings: 0.0157\n",
      "pool: 0.0080\n",
      "afternoon: -0.0047\n",
      "football: -0.0223\n",
      "evening: -0.0471\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "utilized: 0.6993\n",
      "use: 0.6430\n",
      "utilised: 0.6127\n",
      "used.I: 0.5890\n",
      "commonly: 0.5811\n",
      "employed: 0.5790\n",
      "applied: 0.5766\n",
      "uses: 0.5678\n",
      "interchangeably: 0.5512\n",
      "used.To: 0.5499\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "tactic: 0.3285\n",
      "Marketed: 0.3284\n",
      "insed: 0.3283\n",
      "haveused: 0.3283\n",
      "exlained: 0.3283\n",
      "convered: 0.3282\n",
      "visualised: 0.3282\n",
      "swiped: 0.3281\n",
      "lended: 0.3280\n",
      "accuratley: 0.3280\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: friend\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "friends: 0.7886\n",
      "friendly: 0.6158\n",
      "fries: 0.4828\n",
      "ends: 0.4418\n",
      "hand: 0.4350\n",
      "arrived: 0.4317\n",
      "ginger: 0.4305\n",
      "found: 0.4159\n",
      "fried: 0.4150\n",
      "entree: 0.4146\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "whole: 0.0405\n",
      "lovely: 0.0400\n",
      "choice: 0.0397\n",
      "staff: 0.0390\n",
      "wifi: 0.0366\n",
      "work: 0.0266\n",
      "enough: 0.0260\n",
      "watch: 0.0166\n",
      "would: 0.0140\n",
      "pool: -0.0360\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "co-worker: 0.7279\n",
      "colleague: 0.7211\n",
      "coworker: 0.7045\n",
      "buddy: 0.6975\n",
      "friends: 0.6955\n",
      "freind: 0.6946\n",
      "firend: 0.6916\n",
      "schoolmate: 0.6802\n",
      "cousin: 0.6767\n",
      "frined: 0.6730\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "darlingest: 0.3838\n",
      "hallmates: 0.3838\n",
      "co-blogging: 0.3838\n",
      "son.A: 0.3837\n",
      "foster-brother: 0.3837\n",
      "art-dealer: 0.3836\n",
      "bride-to-be: 0.3834\n",
      "hubsters: 0.3833\n",
      "fiances: 0.3833\n",
      "brother.But: 0.3833\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: courtyard\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "sunny: 0.6444\n",
      "cozy: 0.6294\n",
      "views: 0.5814\n",
      "floor: 0.5743\n",
      "empty: 0.5703\n",
      "cooked: 0.5448\n",
      "outdoor: 0.5289\n",
      "cute: 0.5251\n",
      "board: 0.5196\n",
      "comfortable: 0.5133\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "they: 0.1114\n",
      "theyll: 0.1107\n",
      "ever: 0.1057\n",
      "order: 0.0962\n",
      "down: 0.0943\n",
      "usually: 0.0871\n",
      "never: 0.0847\n",
      "again: 0.0715\n",
      "dont: 0.0570\n",
      "theyre: 0.0562\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "courtyards: 0.7709\n",
      "courtyard.The: 0.7686\n",
      "courtyard.: 0.7039\n",
      "court-yard: 0.7023\n",
      "couryard: 0.6866\n",
      "terrace: 0.6791\n",
      "coutyard: 0.6762\n",
      "veranda: 0.6599\n",
      "courtyard-style: 0.6553\n",
      "courtyard-: 0.6552\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "sunfilled: 0.4199\n",
      "master-bedroom: 0.4198\n",
      "25sqm: 0.4197\n",
      "garden-type: 0.4197\n",
      "fortresslike: 0.4197\n",
      "five-floor: 0.4195\n",
      "semicircular: 0.4195\n",
      "caravansary: 0.4195\n",
      "shrine.The: 0.4194\n",
      "theentrance: 0.4194\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: college\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "football: 0.5454\n",
      "dancing: 0.5402\n",
      "crowd: 0.5153\n",
      "cost: 0.5063\n",
      "cozy: 0.5003\n",
      "irish: 0.4973\n",
      "center: 0.4965\n",
      "holiday: 0.4871\n",
      "venue: 0.4775\n",
      "beignets: 0.4755\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "meal: 0.0665\n",
      "things: 0.0631\n",
      "reviews: 0.0583\n",
      "really: 0.0533\n",
      "prices: 0.0497\n",
      "them: 0.0483\n",
      "told: 0.0403\n",
      "tasted: 0.0181\n",
      "ordered: 0.0097\n",
      "very: -0.0280\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "university: 0.7493\n",
      "colleg: 0.6951\n",
      "graduate: 0.6883\n",
      "school: 0.6803\n",
      "colllege: 0.6674\n",
      "high-school: 0.6658\n",
      "grad: 0.6632\n",
      "colleges: 0.6590\n",
      "student: 0.6506\n",
      "colege: 0.6492\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "college-readiness: 0.3953\n",
      "teen: 0.3952\n",
      "post-football: 0.3952\n",
      "batchmates: 0.3951\n",
      "Pre-university: 0.3951\n",
      "WyoTech: 0.3951\n",
      "student-managed: 0.3951\n",
      "B.G.S.: 0.3951\n",
      "acads: 0.3950\n",
      "postbac: 0.3950\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: bottomless\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "gotta: 0.6257\n",
      "less: 0.5481\n",
      "biscuits: 0.5474\n",
      "bowl: 0.5470\n",
      "fabulous: 0.5443\n",
      "almost: 0.5140\n",
      "mimosas: 0.5130\n",
      "charbroiled: 0.5031\n",
      "catch: 0.4994\n",
      "guess: 0.4955\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "with: 0.0590\n",
      "need: 0.0529\n",
      "worth: 0.0506\n",
      "last: 0.0489\n",
      "right: 0.0483\n",
      "trying: 0.0453\n",
      "work: 0.0235\n",
      "show: 0.0186\n",
      "typical: 0.0090\n",
      "youre: -0.0031\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "Bottomless: 0.6386\n",
      "fathomless: 0.5682\n",
      "inexhaustible: 0.5409\n",
      "abyss: 0.5312\n",
      "pit: 0.5115\n",
      "limitless: 0.5096\n",
      "unending: 0.5061\n",
      "endless: 0.5013\n",
      "ever-deepening: 0.4884\n",
      "depthless: 0.4800\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "Knee-deep: 0.3194\n",
      "40-ounce: 0.3192\n",
      "soul-crushing: 0.3192\n",
      "dizzying: 0.3192\n",
      "incomprehensibility: 0.3192\n",
      "Olympic-size: 0.3192\n",
      "not-so-hidden: 0.3192\n",
      "dumping: 0.3192\n",
      "trough-like: 0.3192\n",
      "metre-deep: 0.3191\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: words\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "lasagna: 0.6411\n",
      "world: 0.5374\n",
      "idea: 0.5165\n",
      "worth: 0.4913\n",
      "worst: 0.4702\n",
      "crab: 0.4681\n",
      "slice: 0.4656\n",
      "work: 0.4626\n",
      "foot: 0.4607\n",
      "products: 0.4534\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "rolls: 0.0631\n",
      "manager: 0.0631\n",
      "some: 0.0629\n",
      "same: 0.0556\n",
      "other: 0.0514\n",
      "bartender: 0.0505\n",
      "brunch: 0.0392\n",
      "fresh: 0.0294\n",
      "above: 0.0199\n",
      "night: -0.0352\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "phrases: 0.7432\n",
      "words.In: 0.7173\n",
      "word: 0.7040\n",
      "words.And: 0.6966\n",
      "words.If: 0.6904\n",
      "words.The: 0.6831\n",
      "words-: 0.6798\n",
      "words.That: 0.6765\n",
      "words--the: 0.6726\n",
      "words.There: 0.6710\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "lettter: 0.3682\n",
      "coloquial: 0.3682\n",
      "Vocabulary: 0.3681\n",
      "epigraphs: 0.3681\n",
      "sub-texts: 0.3681\n",
      "J-O-B-S: 0.3680\n",
      "grammatically: 0.3678\n",
      "exhortation: 0.3678\n",
      "italicizing: 0.3678\n",
      "ineloquently: 0.3678\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: charge\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "cash: 0.5017\n",
      "clear: 0.4755\n",
      "karaoke: 0.4730\n",
      "appointment: 0.4678\n",
      "number: 0.4603\n",
      "charbroiled: 0.4541\n",
      "book: 0.4507\n",
      "credit: 0.4502\n",
      "cards: 0.4479\n",
      "forever: 0.4390\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "looks: 0.0349\n",
      "seasoned: 0.0339\n",
      "stop: 0.0339\n",
      "salmon: 0.0290\n",
      "rolls: 0.0210\n",
      "roll: 0.0156\n",
      "potato: 0.0093\n",
      "tasty: -0.0056\n",
      "shrimp: -0.0429\n",
      "pork: -0.0592\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "charged: 0.7045\n",
      "charges: 0.6959\n",
      "charging: 0.6665\n",
      "charge.The: 0.6277\n",
      "charge.It: 0.6253\n",
      "charge.When: 0.6197\n",
      "charge.A: 0.6143\n",
      "charge.: 0.6136\n",
      "charge.In: 0.6125\n",
      "charge.If: 0.6076\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "unmetered: 0.2871\n",
      "capacitances: 0.2871\n",
      "1.00: 0.2870\n",
      "dditional: 0.2870\n",
      "hipping: 0.2870\n",
      "fee.4.: 0.2870\n",
      "cancelation: 0.2870\n",
      "on-request: 0.2869\n",
      "dues: 0.2868\n",
      "re-serve: 0.2868\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: quality\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "sell: 0.4239\n",
      "alternative: 0.4186\n",
      "mexican: 0.4093\n",
      "several: 0.4033\n",
      "croissants: 0.4006\n",
      "true: 0.3964\n",
      "quite: 0.3829\n",
      "stars: 0.3828\n",
      "city: 0.3823\n",
      "stale: 0.3792\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "came: 0.0212\n",
      "sandwiches: 0.0205\n",
      "foods: 0.0152\n",
      "night: 0.0083\n",
      "almost: 0.0042\n",
      "bottle: 0.0003\n",
      "sample: -0.0022\n",
      "vodka: -0.0171\n",
      "different: -0.0179\n",
      "drink: -0.0468\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "qaulity: 0.7577\n",
      "quailty: 0.7467\n",
      "qualty: 0.7218\n",
      "qualiy: 0.7139\n",
      "qualitiy: 0.6912\n",
      "quaility: 0.6892\n",
      "quaity: 0.6882\n",
      "qulaity: 0.6862\n",
      "quality.This: 0.6804\n",
      "quallity: 0.6790\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "made-in-Italy: 0.3486\n",
      "products.Product: 0.3485\n",
      "better-made: 0.3485\n",
      "biocompatibility: 0.3484\n",
      "credibility: 0.3483\n",
      "customer: 0.3483\n",
      "service.All: 0.3483\n",
      "moneyOverall: 0.3482\n",
      "OEM-equivalent: 0.3482\n",
      "ensures: 0.3482\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: theres\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "heres: 0.7657\n",
      "there: 0.5406\n",
      "discount: 0.4528\n",
      "indy: 0.4221\n",
      "together: 0.4111\n",
      "wednesdays: 0.4102\n",
      "places: 0.3946\n",
      "shes: 0.3820\n",
      "forever: 0.3780\n",
      "pick: 0.3752\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "soup: 0.0139\n",
      "portions: 0.0107\n",
      "lots: 0.0093\n",
      "delicious: 0.0089\n",
      "expensive: 0.0085\n",
      "kids: -0.0012\n",
      "curry: -0.0021\n",
      "local: -0.0170\n",
      "menu: -0.0210\n",
      "eggs: -0.0874\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "Theres: 0.8556\n",
      "thats: 0.7775\n",
      "isnt: 0.7259\n",
      "therell: 0.7129\n",
      "tehre: 0.7126\n",
      "dont: 0.6912\n",
      "THere: 0.6733\n",
      "whats: 0.6723\n",
      "cant: 0.6659\n",
      "aint: 0.6650\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "deffinently: 0.4944\n",
      "THe: 0.4944\n",
      "ypu: 0.4944\n",
      "IMO.: 0.4944\n",
      "doeas: 0.4944\n",
      "agian: 0.4944\n",
      "thatz: 0.4943\n",
      "prbly: 0.4943\n",
      "alredy: 0.4943\n",
      "ony: 0.4942\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: terrible\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "horrible: 0.7164\n",
      "sucks: 0.4771\n",
      "warning: 0.4762\n",
      "interesting: 0.4687\n",
      "pleasant: 0.4647\n",
      "poor: 0.4634\n",
      "services: 0.4574\n",
      "nasty: 0.4521\n",
      "valet: 0.4478\n",
      "whole: 0.4449\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "heard: 0.0312\n",
      "gelato: 0.0288\n",
      "sandwich: 0.0269\n",
      "heaven: 0.0234\n",
      "italian: 0.0230\n",
      "didnt: 0.0202\n",
      "chocolate: 0.0113\n",
      "youve: 0.0095\n",
      "milkshakes: -0.0016\n",
      "isnt: -0.0190\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "horrible: 0.9434\n",
      "awful: 0.8681\n",
      "horrendous: 0.8550\n",
      "dreadful: 0.8397\n",
      "horrid: 0.8354\n",
      "atrocious: 0.7808\n",
      "aweful: 0.7465\n",
      "terrrible: 0.7392\n",
      "bad: 0.7317\n",
      "TERRIBLE: 0.7182\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "all-too-well: 0.4066\n",
      "overpriced: 0.4066\n",
      "soul-sapping: 0.4065\n",
      "alarming: 0.4064\n",
      "overhyped: 0.4064\n",
      "bearable: 0.4064\n",
      "letdowns: 0.4064\n",
      "inconsitent: 0.4063\n",
      "friggen: 0.4063\n",
      "groan-worthy: 0.4062\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: monday\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "mondays: 0.7349\n",
      "sunday: 0.5262\n",
      "thursday: 0.5057\n",
      "friday: 0.4859\n",
      "football: 0.4800\n",
      "tuesday: 0.4547\n",
      "months: 0.4476\n",
      "thursdays: 0.4381\n",
      "wednesday: 0.4329\n",
      "poor: 0.4166\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "board: 0.0240\n",
      "tastes: 0.0235\n",
      "give: 0.0190\n",
      "instead: 0.0188\n",
      "write: 0.0187\n",
      "real: 0.0126\n",
      "heard: 0.0027\n",
      "forward: -0.0083\n",
      "clean: -0.0102\n",
      "hope: -0.0137\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "thursday: 0.8969\n",
      "tuesday: 0.8849\n",
      "friday: 0.8834\n",
      "wednesday: 0.8832\n",
      "saturday: 0.8418\n",
      "sunday: 0.8039\n",
      "thrusday: 0.7686\n",
      "monday-: 0.7546\n",
      "monday.: 0.7447\n",
      "wednsday: 0.7266\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "b.day: 0.4195\n",
      "wohoo: 0.4195\n",
      "poned: 0.4195\n",
      "everyone.i: 0.4195\n",
      "aprill: 0.4194\n",
      "usual.: 0.4192\n",
      "rn.: 0.4190\n",
      "June.On: 0.4189\n",
      "deadline.: 0.4189\n",
      "tisdag: 0.4188\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: watch\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "valet: 0.4572\n",
      "catch: 0.4495\n",
      "take: 0.4252\n",
      "quiet: 0.3936\n",
      "louis: 0.3932\n",
      "yeah: 0.3890\n",
      "baby: 0.3864\n",
      "spot: 0.3848\n",
      "youll: 0.3798\n",
      "clothes: 0.3768\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "beer: 0.0222\n",
      "must: 0.0211\n",
      "curry: 0.0204\n",
      "friend: 0.0166\n",
      "both: 0.0127\n",
      "italian: 0.0125\n",
      "friends: 0.0067\n",
      "baked: 0.0047\n",
      "roasted: 0.0043\n",
      "expect: -0.0018\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "watching: 0.7537\n",
      "watched: 0.6781\n",
      "wathc: 0.6773\n",
      "watch.This: 0.6735\n",
      "watch.I: 0.6727\n",
      "watch.: 0.6710\n",
      "Watch: 0.6667\n",
      "watche: 0.6637\n",
      "watch.If: 0.6611\n",
      "watches: 0.6540\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "filmon: 0.3297\n",
      "channnel: 0.3296\n",
      "entertaing: 0.3296\n",
      "rubberneck: 0.3296\n",
      "amazement: 0.3294\n",
      "television.This: 0.3293\n",
      "out.Click: 0.3293\n",
      "channel.If: 0.3293\n",
      "film.You: 0.3291\n",
      "'ll: 0.3291\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: tables\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "table: 0.6762\n",
      "raining: 0.5083\n",
      "waited: 0.4997\n",
      "diner: 0.4821\n",
      "shes: 0.4757\n",
      "waiting: 0.4722\n",
      "lettuce: 0.4575\n",
      "wanted: 0.4517\n",
      "world: 0.4450\n",
      "worst: 0.4443\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "pasta: 0.0534\n",
      "hole: 0.0522\n",
      "soda: 0.0447\n",
      "coffee: 0.0439\n",
      "show: 0.0412\n",
      "rice: 0.0392\n",
      "excellent: 0.0350\n",
      "deal: 0.0209\n",
      "every: 0.0197\n",
      "look: 0.0057\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "table: 0.8093\n",
      "tables.The: 0.7333\n",
      "tables.I: 0.7164\n",
      "tables.: 0.7157\n",
      "Tables: 0.7049\n",
      "tables.This: 0.6788\n",
      "tabels: 0.6637\n",
      "tables-: 0.6597\n",
      "tables.We: 0.6416\n",
      "sub-tables: 0.6354\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "column: 0.3213\n",
      "diner-style: 0.3213\n",
      "sunbeds: 0.3211\n",
      "stackable: 0.3210\n",
      "teradata: 0.3210\n",
      "chalk-board: 0.3208\n",
      "gazeebo: 0.3207\n",
      "tables.Thermal: 0.3207\n",
      "desks.: 0.3207\n",
      "dressers.Victorian: 0.3207\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: their\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "christmas: 0.3559\n",
      "pickup: 0.3356\n",
      "skip: 0.3284\n",
      "youll: 0.3280\n",
      "margaritas: 0.3223\n",
      "website: 0.3180\n",
      "karaoke: 0.3074\n",
      "there: 0.3071\n",
      "youve: 0.3040\n",
      "margarita: 0.3025\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "kind: 0.0013\n",
      "because: -0.0006\n",
      "find: -0.0009\n",
      "again: -0.0042\n",
      "bucks: -0.0061\n",
      "sitting: -0.0086\n",
      "sucks: -0.0206\n",
      "able: -0.0220\n",
      "incredible: -0.0334\n",
      "available: -0.0337\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "thier: 0.6990\n",
      "ttheir: 0.6861\n",
      "ththeir: 0.6828\n",
      "own: 0.6812\n",
      "thheir: 0.6693\n",
      "theirr: 0.6599\n",
      "theur: 0.6557\n",
      "thieir: 0.6399\n",
      "them: 0.6393\n",
      "themselves: 0.6364\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "impending: 0.3332\n",
      "countrys: 0.3330\n",
      "thsing: 0.3329\n",
      "outsides: 0.3329\n",
      "foisting: 0.3328\n",
      "them.Back: 0.3328\n",
      "THey: 0.3327\n",
      "things.They: 0.3327\n",
      "lives.To: 0.3326\n",
      "progress.They: 0.3326\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: appetizer\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "appetizers: 0.8939\n",
      "feeling: 0.5267\n",
      "apple: 0.5044\n",
      "filling: 0.4486\n",
      "dumplings: 0.4381\n",
      "feel: 0.4355\n",
      "already: 0.4259\n",
      "falafel: 0.4259\n",
      "chicago: 0.4252\n",
      "almost: 0.4241\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "during: 0.0393\n",
      "sale: 0.0364\n",
      "music: 0.0298\n",
      "large: 0.0254\n",
      "small: 0.0240\n",
      "that: 0.0120\n",
      "than: 0.0058\n",
      "where: -0.0006\n",
      "wait: -0.0193\n",
      "free: -0.0214\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "appetizers: 0.8346\n",
      "entree: 0.7567\n",
      "appetiser: 0.7545\n",
      "appitizer: 0.7376\n",
      "entreé: 0.6983\n",
      "entrée: 0.6978\n",
      "antipasto: 0.6924\n",
      "Appetizer: 0.6882\n",
      "appetizer-sized: 0.6862\n",
      "bruschetta: 0.6740\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "scallops: 0.4193\n",
      "kebob: 0.4191\n",
      "omelette: 0.4191\n",
      "tangine: 0.4191\n",
      "shrimp-: 0.4191\n",
      "polpette: 0.4189\n",
      "omelete: 0.4189\n",
      "menu.This: 0.4189\n",
      "Sangria: 0.4189\n",
      "platter.: 0.4188\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: draft\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "craft: 0.6590\n",
      "gift: 0.4935\n",
      "cozy: 0.4934\n",
      "courtyard: 0.4850\n",
      "floor: 0.4750\n",
      "checking: 0.4700\n",
      "gluten: 0.4600\n",
      "byob: 0.4500\n",
      "mini: 0.4468\n",
      "checkin: 0.4361\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "these: 0.0419\n",
      "hands: 0.0350\n",
      "want: 0.0334\n",
      "thats: 0.0276\n",
      "maybe: 0.0168\n",
      "tampa: 0.0085\n",
      "downtown: 0.0043\n",
      "last: 0.0042\n",
      "than: -0.0063\n",
      "just: -0.0077\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "Draft: 0.7870\n",
      "drafts: 0.7854\n",
      "draft.The: 0.7521\n",
      "draft.: 0.7252\n",
      "drafted: 0.7243\n",
      "draft-: 0.7084\n",
      "draft.I: 0.7074\n",
      "drafting: 0.6986\n",
      "draft.It: 0.6888\n",
      "draft.But: 0.6765\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "cover-2: 0.3220\n",
      "projectability: 0.3219\n",
      "DEs: 0.3218\n",
      "NFL.He: 0.3218\n",
      "Ugoh: 0.3217\n",
      "OMFL: 0.3217\n",
      "Jan-Reg: 0.3216\n",
      "31-page: 0.3216\n",
      "14-page: 0.3216\n",
      "prospect.The: 0.3216\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: wanted\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "wasnt: 0.6151\n",
      "want: 0.6040\n",
      "waited: 0.5806\n",
      "yeah: 0.5780\n",
      "waste: 0.5683\n",
      "started: 0.5476\n",
      "stopped: 0.5163\n",
      "while: 0.5041\n",
      "yourself: 0.5025\n",
      "unless: 0.4909\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "beer: 0.0769\n",
      "favorite: 0.0630\n",
      "half: 0.0583\n",
      "afternoon: 0.0574\n",
      "beers: 0.0536\n",
      "feel: 0.0530\n",
      "awesome: 0.0464\n",
      "drinks: 0.0446\n",
      "love: 0.0298\n",
      "cool: 0.0220\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "decided: 0.7636\n",
      "knew: 0.7332\n",
      "wished: 0.7136\n",
      "did: 0.6823\n",
      "chose: 0.6661\n",
      "didn: 0.6629\n",
      "want: 0.6601\n",
      "hoped: 0.6561\n",
      "thought: 0.6473\n",
      "liked: 0.6467\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "excited.So: 0.3603\n",
      "lvoed: 0.3602\n",
      "disinclined: 0.3602\n",
      "apprehensive: 0.3602\n",
      "loooked: 0.3600\n",
      "anybody: 0.3600\n",
      "try.We: 0.3599\n",
      "instinctively: 0.3599\n",
      "it.Eventually: 0.3599\n",
      "was.If: 0.3598\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: pack\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "snack: 0.6660\n",
      "packed: 0.5501\n",
      "movie: 0.5212\n",
      "wanted: 0.4811\n",
      "baby: 0.4712\n",
      "karaoke: 0.4630\n",
      "without: 0.4562\n",
      "gelato: 0.4533\n",
      "pass: 0.4516\n",
      "toast: 0.4467\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "stale: 0.0548\n",
      "location: 0.0545\n",
      "available: 0.0325\n",
      "worth: 0.0294\n",
      "outdoor: 0.0203\n",
      "venue: 0.0124\n",
      "outside: -0.0008\n",
      "side: -0.0179\n",
      "inside: -0.0296\n",
      "guess: -0.0366\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "packs: 0.8497\n",
      "pack.The: 0.7139\n",
      "pack.This: 0.7026\n",
      "pack.: 0.7025\n",
      "pack.It: 0.6977\n",
      "pack.I: 0.6927\n",
      "packs.The: 0.6643\n",
      "pack.If: 0.6529\n",
      "Pack: 0.6527\n",
      "packs.: 0.6175\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "three-pound: 0.3160\n",
      "stickers: 0.3160\n",
      "Lipoly: 0.3160\n",
      "two-pound: 0.3160\n",
      "sticker: 0.3159\n",
      "35lb: 0.3159\n",
      "20kgs: 0.3158\n",
      "handy: 0.3158\n",
      "7oz.: 0.3158\n",
      "rubberbanded: 0.3156\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: theyre\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "theyll: 0.4616\n",
      "school: 0.4297\n",
      "tasting: 0.4063\n",
      "anyone: 0.4027\n",
      "life: 0.3961\n",
      "stopped: 0.3801\n",
      "taken: 0.3774\n",
      "foot: 0.3701\n",
      "nights: 0.3682\n",
      "christmas: 0.3670\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "cooked: 0.0029\n",
      "beers: 0.0009\n",
      "fresh: 0.0001\n",
      "awesome: -0.0002\n",
      "catfish: -0.0007\n",
      "outside: -0.0010\n",
      "excellent: -0.0037\n",
      "first: -0.0046\n",
      "date: -0.0109\n",
      "cant: -0.0942\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "Theyre: 0.8187\n",
      "arent: 0.7898\n",
      "theyll: 0.7636\n",
      "arnt: 0.7334\n",
      "theyve: 0.7313\n",
      "youre: 0.7265\n",
      "isnt: 0.6631\n",
      "thats: 0.6608\n",
      "werent: 0.6581\n",
      "Theyve: 0.6493\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "miight: 0.4158\n",
      "epecially: 0.4157\n",
      "great.: 0.4157\n",
      "goood: 0.4157\n",
      "cluless: 0.4156\n",
      "worrie: 0.4156\n",
      "morons.: 0.4156\n",
      "likely.: 0.4156\n",
      "likethey: 0.4156\n",
      "ofcouse: 0.4155\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: month\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "months: 0.7569\n",
      "move: 0.5183\n",
      "second: 0.5130\n",
      "alcohol: 0.4910\n",
      "offer: 0.4657\n",
      "stopped: 0.4441\n",
      "note: 0.4351\n",
      "snack: 0.4108\n",
      "kinda: 0.4100\n",
      "seems: 0.3995\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "large: 0.0306\n",
      "best: 0.0242\n",
      "pasta: 0.0194\n",
      "bean: 0.0159\n",
      "clean: 0.0140\n",
      "cute: 0.0084\n",
      "excellent: 0.0048\n",
      "beans: 0.0029\n",
      "weeks: -0.0214\n",
      "cook: -0.0225\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "week: 0.8289\n",
      "year: 0.7769\n",
      "months: 0.7393\n",
      "month.This: 0.7287\n",
      "weeks: 0.7160\n",
      "month.So: 0.7077\n",
      "month.Now: 0.6829\n",
      "month.As: 0.6773\n",
      "month.Since: 0.6736\n",
      "month.It: 0.6729\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "1-week: 0.4274\n",
      "4week: 0.4272\n",
      "December.We: 0.4271\n",
      "March1: 0.4271\n",
      "year.Maybe: 0.4271\n",
      "October.For: 0.4270\n",
      "Wednesday.So: 0.4268\n",
      "Sunday: 0.4267\n",
      "five: 0.4264\n",
      "Earlier: 0.4264\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define a function to find top similar and dissimilar words\n",
    "def find_top_n(word, word_list, model, n=10):\n",
    "    similarities = model.wv.most_similar(word, topn=n)\n",
    "    return similarities\n",
    "\n",
    "# Loop to process words\n",
    "for word in random.sample(words, 20):  # Take 20 random words from 'words' list\n",
    "    print(f\"Analyzing word: {word}\\n\")  # Print the word being analyzed\n",
    "\n",
    "    # Custom Model\n",
    "    similar_custom = find_top_n(word, words, fast_Text_model)\n",
    "    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "    \n",
    "    # Pretrained Model\n",
    "    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n",
    "    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "    \n",
    "    # Print the output\n",
    "    print(\"Top 10 similar words (custom model):\")\n",
    "    for similar_word, similarity in similar_custom:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 opposite words (custom model):\")\n",
    "    for opposite_word, similarity in dissimilar_custom:\n",
    "        print(f\"{opposite_word}: {similarity:.4f}\")\n",
    "\n",
    "    print(\"\\nTop 10 similar words (pre-trained model):\")\n",
    "    for similar_word, similarity in similar_pretrained:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    print(\"\\nTop 10 opposite words (pre-trained model):\")\n",
    "    for opposite_word, similarity in dissimilar_pretrained:\n",
    "        print(f\"{opposite_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b1af2",
   "metadata": {
    "papermill": {
     "duration": 0.01802,
     "end_time": "2024-04-22T10:07:14.735096",
     "exception": false,
     "start_time": "2024-04-22T10:07:14.717076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Writting the results in pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5628ca95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:14.773882Z",
     "iopub.status.busy": "2024-04-22T10:07:14.773349Z",
     "iopub.status.idle": "2024-04-22T10:07:22.433594Z",
     "shell.execute_reply": "2024-04-22T10:07:22.432679Z"
    },
    "papermill": {
     "duration": 7.682882,
     "end_time": "2024-04-22T10:07:22.436300",
     "exception": false,
     "start_time": "2024-04-22T10:07:14.753418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "import random\n",
    "\n",
    "# Create a PDF document\n",
    "pdf = SimpleDocTemplate(\"word_analysis_results.pdf\", pagesize=letter)\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Define a function to find top similar and dissimilar words\n",
    "def find_top_n(word, word_list, model, n=10):\n",
    "    similarities = model.wv.most_similar(word, topn=n)\n",
    "    return similarities\n",
    "\n",
    "# Define a function to write results to the PDF\n",
    "def write_to_pdf(pdf, content_list):\n",
    "    # Create paragraphs for each content item\n",
    "    content = [Paragraph(item, styles[\"Normal\"]) for item in content_list]\n",
    "    pdf.build(content)\n",
    "\n",
    "# List to store analysis results\n",
    "analysis_results = []\n",
    "\n",
    "# Loop to process words\n",
    "for word in random.sample(words, 20):  # Take 20 random words from 'words' list\n",
    "    analysis_results.append(f\"Analyzing word: {word}\\n\")  # Add the word being analyzed\n",
    "\n",
    "    # Custom Model\n",
    "    similar_custom = find_top_n(word, words, fast_Text_model)\n",
    "    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "\n",
    "    # Pretrained Model\n",
    "    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n",
    "    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "\n",
    "    # Add analysis results to the list\n",
    "    analysis_results.append(\"Top 10 similar words (custom model):\")\n",
    "    for similar_word, similarity in similar_custom:\n",
    "        analysis_results.append(f\"{similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results.append(\"\\nTop 10 opposite words (custom model):\")\n",
    "    for opposite_word, similarity in dissimilar_custom:\n",
    "        analysis_results.append(f\"{opposite_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results.append(\"\\nTop 10 similar words (pre-trained model):\")\n",
    "    for similar_word, similarity in similar_pretrained:\n",
    "        analysis_results.append(f\"{similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results.append(\"\\nTop 10 opposite words (pre-trained model):\")\n",
    "    for opposite_word, similarity in dissimilar_pretrained:\n",
    "        analysis_results.append(f\"{opposite_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results.append(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d34c48",
   "metadata": {
    "papermill": {
     "duration": 0.018352,
     "end_time": "2024-04-22T10:07:22.476437",
     "exception": false,
     "start_time": "2024-04-22T10:07:22.458085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72459f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:22.516393Z",
     "iopub.status.busy": "2024-04-22T10:07:22.515903Z",
     "iopub.status.idle": "2024-04-22T10:07:22.711937Z",
     "shell.execute_reply": "2024-04-22T10:07:22.709881Z"
    },
    "papermill": {
     "duration": 0.219738,
     "end_time": "2024-04-22T10:07:22.715100",
     "exception": false,
     "start_time": "2024-04-22T10:07:22.495362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add conclusion\n",
    "conclusion = [\n",
    "    \"\\n\\nConclusion:\",\n",
    "    \"In this analysis, we explored the similarity and dissimilarity of words using both a custom FastText model and a pre-trained FastText model.\",\n",
    "    \"We found that at some words the pretrained model works better than the custom model as it really give very accurate results but at some other words the pretrained model just gives different forms of the same given word but the custom model gives diffrent words most of them are close to thegiven word meaning .\",\n",
    "    \"Overall, the results indicate that two models works pretty good but its word dependent if the word is rare or not in the pre-trained model's vocabulary, it may not perform well. \"\n",
    "]\n",
    "\n",
    "# Append conclusion to analysis_results\n",
    "analysis_results += conclusion\n",
    "\n",
    "# Write the analysis results to the PDF\n",
    "write_to_pdf(pdf, analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8f82b",
   "metadata": {
    "papermill": {
     "duration": 0.014234,
     "end_time": "2024-04-22T10:07:22.743894",
     "exception": false,
     "start_time": "2024-04-22T10:07:22.729660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Different way for writing in pdf using new different words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693baeab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:22.776866Z",
     "iopub.status.busy": "2024-04-22T10:07:22.775958Z",
     "iopub.status.idle": "2024-04-22T10:07:29.987540Z",
     "shell.execute_reply": "2024-04-22T10:07:29.986641Z"
    },
    "papermill": {
     "duration": 7.231737,
     "end_time": "2024-04-22T10:07:29.990665",
     "exception": false,
     "start_time": "2024-04-22T10:07:22.758928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def write_to_pdf(pdf_file, analysis_results_custom, analysis_results_pretrained):\n",
    "    c = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "    y = 750  # Starting y position\n",
    "    x_custom = 50  # Starting x position for custom model\n",
    "    x_pretrained = 350  # Starting x position for pretrained model\n",
    "    line_spacing = 15  # Spacing between lines\n",
    "    page_height = 800  # Height of the page\n",
    "    bottom_margin = 50  # Bottom margin\n",
    "\n",
    "    def check_new_page():\n",
    "        nonlocal y\n",
    "        if y < bottom_margin:\n",
    "            c.showPage()\n",
    "            y = page_height\n",
    "\n",
    "    for custom_line, pretrained_line in zip(analysis_results_custom, analysis_results_pretrained):\n",
    "        c.drawString(x_custom, y, custom_line)\n",
    "        c.drawString(x_pretrained, y, pretrained_line)\n",
    "        y -= line_spacing  # Adjust y position for next line\n",
    "        check_new_page()\n",
    "\n",
    "    c.save()\n",
    "\n",
    "# Usage\n",
    "pdf_file = \"analysis_results.pdf\"\n",
    "# Define a function to find top similar and dissimilar words\n",
    "def find_top_n(word, word_list, model, n=10):\n",
    "    similarities = model.wv.most_similar(word, topn=n)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# List to store analysis results\n",
    "analysis_results_custom = []\n",
    "analysis_results_pretrained = []\n",
    "\n",
    "# Loop to process words\n",
    "for word in random.sample(words, 20):  # Take 20 random words from 'words' list\n",
    "    analysis_results_custom.append(f\"Analyzing word: {word}\\n\")  # Add the word being analyzed\n",
    "    analysis_results_pretrained.append(f\"Analyzing word: {word}\\n\")\n",
    "\n",
    "    # Custom Model\n",
    "    similar_custom = find_top_n(word, words, fast_Text_model)\n",
    "    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "\n",
    "    # Pretrained Model\n",
    "    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n",
    "    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "\n",
    "    # Add analysis results to the list\n",
    "    analysis_results_custom.append(\"Top 10 similar words (custom model):\")\n",
    "    for similar_word, similarity in similar_custom:\n",
    "        analysis_results_custom.append(f\"{similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results_custom.append(\"\\nTop 10 opposite words (custom model):\")\n",
    "    for opposite_word, similarity in dissimilar_custom:\n",
    "        analysis_results_custom.append(f\"{opposite_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results_pretrained.append(\"\\nTop 10 similar words (pre-trained model):\")\n",
    "    for similar_word, similarity in similar_pretrained:\n",
    "        analysis_results_pretrained.append(f\"{similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results_pretrained.append(\"\\nTop 10 opposite words (pre-trained model):\")\n",
    "    for opposite_word, similarity in dissimilar_pretrained:\n",
    "        analysis_results_pretrained.append(f\"{opposite_word}: {similarity:.4f}\")\n",
    "\n",
    "    analysis_results_pretrained.append(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n",
    "    analysis_results_custom.append(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "write_to_pdf(pdf_file, analysis_results_custom, analysis_results_pretrained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46339db1",
   "metadata": {
    "papermill": {
     "duration": 0.018156,
     "end_time": "2024-04-22T10:07:30.027639",
     "exception": false,
     "start_time": "2024-04-22T10:07:30.009483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# trying unseen random words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d793f5a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T10:07:30.061810Z",
     "iopub.status.busy": "2024-04-22T10:07:30.061287Z",
     "iopub.status.idle": "2024-04-22T10:07:32.291503Z",
     "shell.execute_reply": "2024-04-22T10:07:32.287685Z"
    },
    "papermill": {
     "duration": 2.248863,
     "end_time": "2024-04-22T10:07:32.294054",
     "exception": false,
     "start_time": "2024-04-22T10:07:30.045191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing word: hi\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "highly: 0.7356\n",
      "sushi: 0.7315\n",
      "high: 0.7217\n",
      "recommended: 0.4410\n",
      "split: 0.4205\n",
      "beware: 0.4078\n",
      "website: 0.4043\n",
      "priced: 0.3964\n",
      "recommend: 0.3942\n",
      "delivery: 0.3800\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "doing: -0.0124\n",
      "most: -0.0131\n",
      "sign: -0.0169\n",
      "leave: -0.0195\n",
      "pickup: -0.0252\n",
      "located: -0.0270\n",
      "stout: -0.0281\n",
      "without: -0.0385\n",
      "next: -0.0390\n",
      "move: -0.0458\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "hi.: 0.7208\n",
      "hello: 0.7038\n",
      "Hi: 0.6973\n",
      "hello.: 0.6432\n",
      "hi-: 0.6391\n",
      "hiii: 0.6334\n",
      "hiiiii: 0.6148\n",
      "Hi.: 0.6143\n",
      "Hi-: 0.6103\n",
      "hellow: 0.6086\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "iwr: 0.4103\n",
      "uuh: 0.4102\n",
      "iiy: 0.4102\n",
      "jL: 0.4101\n",
      "loL: 0.4101\n",
      "wh: 0.4100\n",
      "mso: 0.4100\n",
      "idont: 0.4100\n",
      "amanda.: 0.4099\n",
      "heI: 0.4099\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: university\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "city: 0.6594\n",
      "quality: 0.6085\n",
      "unfortunately: 0.6005\n",
      "seemed: 0.5812\n",
      "trust: 0.5786\n",
      "variety: 0.5767\n",
      "unique: 0.5743\n",
      "asian: 0.5672\n",
      "true: 0.5638\n",
      "wanted: 0.5559\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "cant: 0.2186\n",
      "door: 0.2160\n",
      "coffee: 0.2141\n",
      "when: 0.2120\n",
      "beef: 0.2112\n",
      "check: 0.2109\n",
      "awesome: 0.2100\n",
      "close: 0.2076\n",
      "cold: 0.2008\n",
      "least: 0.1842\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "univeristy: 0.7531\n",
      "college: 0.7493\n",
      "univesity: 0.7314\n",
      "universities: 0.7240\n",
      "universtiy: 0.7224\n",
      "unversity: 0.7033\n",
      "univerity: 0.6884\n",
      "campus: 0.6862\n",
      "univerisity: 0.6755\n",
      "university.The: 0.6607\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "University.A: 0.4032\n",
      "OUSU: 0.4031\n",
      "polytechnique: 0.4031\n",
      "school-system: 0.4031\n",
      "ShanghaiTech: 0.4031\n",
      "teacher-preparation: 0.4030\n",
      "graduates.: 0.4030\n",
      "UPNG: 0.4030\n",
      "college-sponsored: 0.4029\n",
      "NTU: 0.4029\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: college\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "football: 0.5454\n",
      "dancing: 0.5402\n",
      "crowd: 0.5153\n",
      "cost: 0.5063\n",
      "cozy: 0.5003\n",
      "irish: 0.4973\n",
      "center: 0.4965\n",
      "holiday: 0.4871\n",
      "venue: 0.4775\n",
      "beignets: 0.4755\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "meal: 0.0665\n",
      "things: 0.0631\n",
      "reviews: 0.0583\n",
      "really: 0.0533\n",
      "prices: 0.0497\n",
      "them: 0.0483\n",
      "told: 0.0403\n",
      "tasted: 0.0181\n",
      "ordered: 0.0097\n",
      "very: -0.0280\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "university: 0.7493\n",
      "colleg: 0.6951\n",
      "graduate: 0.6883\n",
      "school: 0.6803\n",
      "colllege: 0.6674\n",
      "high-school: 0.6658\n",
      "grad: 0.6632\n",
      "colleges: 0.6590\n",
      "student: 0.6506\n",
      "colege: 0.6492\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "college-readiness: 0.3953\n",
      "teen: 0.3952\n",
      "post-football: 0.3952\n",
      "batchmates: 0.3951\n",
      "Pre-university: 0.3951\n",
      "WyoTech: 0.3951\n",
      "student-managed: 0.3951\n",
      "B.G.S.: 0.3951\n",
      "acads: 0.3950\n",
      "postbac: 0.3950\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: prepare\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "prepared: 0.9667\n",
      "possibly: 0.4682\n",
      "worst: 0.4660\n",
      "yourself: 0.4591\n",
      "raining: 0.4169\n",
      "type: 0.4011\n",
      "wont: 0.3924\n",
      "tables: 0.3880\n",
      "quite: 0.3827\n",
      "wonderful: 0.3764\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "butter: 0.0207\n",
      "really: 0.0129\n",
      "well: 0.0113\n",
      "ladies: 0.0075\n",
      "sign: 0.0057\n",
      "roast: 0.0033\n",
      "dark: 0.0033\n",
      "honey: -0.0093\n",
      "garlic: -0.0351\n",
      "opinion: -0.0542\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "preparing: 0.7762\n",
      "prepared: 0.7180\n",
      "prepares: 0.6938\n",
      "Prepare: 0.6847\n",
      "preparation: 0.6575\n",
      "pre-prepare: 0.6341\n",
      "perpare: 0.6306\n",
      "Preparing: 0.6064\n",
      "well-prepared: 0.5771\n",
      "over-prepare: 0.5515\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "explain: 0.3164\n",
      "galvanize: 0.3163\n",
      "sneak: 0.3162\n",
      "willneed: 0.3161\n",
      "studying: 0.3160\n",
      "ready.gov: 0.3160\n",
      "demobilize: 0.3160\n",
      "savour: 0.3159\n",
      "retake: 0.3159\n",
      "this--to: 0.3159\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: convert\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "dessert: 0.6899\n",
      "consistent: 0.6843\n",
      "coconut: 0.6686\n",
      "caramel: 0.6108\n",
      "courtyard: 0.6104\n",
      "cozy: 0.6060\n",
      "corn: 0.5921\n",
      "gotta: 0.5862\n",
      "bowl: 0.5834\n",
      "cook: 0.5819\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "there: 0.1705\n",
      "phone: 0.1694\n",
      "work: 0.1656\n",
      "water: 0.1613\n",
      "wasnt: 0.1609\n",
      "wonderful: 0.1577\n",
      "them: 0.1439\n",
      "that: 0.1341\n",
      "this: 0.1254\n",
      "want: 0.1245\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "reconvert: 0.7827\n",
      "converts: 0.7810\n",
      "re-convert: 0.7757\n",
      "converting: 0.7643\n",
      "converted: 0.7360\n",
      "Convert: 0.7062\n",
      "conversion: 0.6642\n",
      "reconverting: 0.6280\n",
      "re-converted: 0.6268\n",
      "Converts: 0.6265\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "vCard: 0.2900\n",
      "integrating: 0.2900\n",
      "parameterise: 0.2900\n",
      "instalmentsYou: 0.2900\n",
      "could: 0.2900\n",
      "Christianty: 0.2900\n",
      "accummulate: 0.2899\n",
      "figure-out: 0.2899\n",
      "synchronise: 0.2899\n",
      "re-sample: 0.2898\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing word: process\n",
      "\n",
      "Top 10 similar words (custom model):\n",
      "less: 0.6560\n",
      "guess: 0.6379\n",
      "unless: 0.6167\n",
      "caramel: 0.5759\n",
      "bottomless: 0.5727\n",
      "groceries: 0.5700\n",
      "gotta: 0.5599\n",
      "products: 0.5533\n",
      "address: 0.5531\n",
      "protein: 0.5471\n",
      "\n",
      "Top 10 opposite words (custom model):\n",
      "friends: 0.1935\n",
      "place: 0.1930\n",
      "friendly: 0.1895\n",
      "here: 0.1789\n",
      "worth: 0.1696\n",
      "show: 0.1674\n",
      "dinner: 0.1510\n",
      "lunch: 0.1346\n",
      "after: 0.1290\n",
      "busy: 0.1208\n",
      "\n",
      "Top 10 similar words (pre-trained model):\n",
      "process.This: 0.7102\n",
      "process.The: 0.7013\n",
      "processes: 0.6925\n",
      "proces: 0.6904\n",
      "process.When: 0.6807\n",
      "process--the: 0.6708\n",
      "process.It: 0.6701\n",
      "process.That: 0.6643\n",
      "process.But: 0.6387\n",
      "process.As: 0.6375\n",
      "\n",
      "Top 10 opposite words (pre-trained model):\n",
      "vote-counting: 0.3377\n",
      "demystifies: 0.3377\n",
      "clinoid: 0.3376\n",
      "re-adjustment: 0.3376\n",
      "deep-drawing: 0.3375\n",
      "complete.As: 0.3375\n",
      "complicating: 0.3374\n",
      "documentation: 0.3374\n",
      "re-tooling: 0.3373\n",
      "extrusion: 0.3373\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def find_top_n(word, word_list, model, n=10):\n",
    "    similarities = model.wv.most_similar(word, topn=n)\n",
    "    return similarities\n",
    "\n",
    "test_words = [\"hi\", \"university\", \"college\", \"prepare\",\"convert\",\"process\"]\n",
    "\n",
    "# Loop to process words\n",
    "for word in test_words:  # Take unseen words from test_words\n",
    "    print(f\"Analyzing word: {word}\\n\")  # Print the word being analyzed\n",
    "\n",
    "    # Custom Model\n",
    "    similar_custom = find_top_n(word, test_words, fast_Text_model)\n",
    "    dissimilar_custom = find_top_n(word, test_words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "    \n",
    "    # Pretrained Model\n",
    "    similar_pretrained = find_top_n(word, test_words, pretrained_fastText_en)\n",
    "    dissimilar_pretrained = find_top_n(word, test_words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n",
    "    \n",
    "    # Print the output\n",
    "    print(\"Top 10 similar words (custom model):\")\n",
    "    for similar_word, similarity in similar_custom:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 opposite words (custom model):\")\n",
    "    for opposite_word, similarity in dissimilar_custom:\n",
    "        print(f\"{opposite_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 similar words (pre-trained model):\")\n",
    "    for similar_word, similarity in similar_pretrained:\n",
    "        print(f\"{similar_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 opposite words (pre-trained model):\")\n",
    "    for opposite_word, similarity in dissimilar_pretrained:\n",
    "        print(f\"{opposite_word}: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 10100,
     "sourceId": 3316532,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 306.387184,
   "end_time": "2024-04-22T10:07:35.789534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-22T10:02:29.402350",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
