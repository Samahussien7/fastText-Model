{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installing reportlab which is used for creating PDFs\n! pip install reportlab","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:18:45.210538Z","iopub.execute_input":"2024-04-22T09:18:45.210995Z","iopub.status.idle":"2024-04-22T09:19:04.715550Z","shell.execute_reply.started":"2024-04-22T09:18:45.210960Z","shell.execute_reply":"2024-04-22T09:19:04.714457Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting reportlab\n  Downloading reportlab-4.2.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from reportlab) (9.5.0)\nCollecting chardet (from reportlab)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading reportlab-4.2.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: chardet, reportlab\nSuccessfully installed chardet-5.2.0 reportlab-4.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nfrom gensim.models.fasttext import FastText # build and train Fast Text model\nfrom gensim.models import Word2Vec # to Load the saved model\nfrom gensim.models.fasttext import load_facebook_model\nfrom tabulate import tabulate\nimport random\nfrom reportlab.lib import colors\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet\n\n# Downloading the pre-trained model from a website\n! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n! gunzip \"cc.en.300.bin.gz\"\nnltk.download('wordnet', \"/kaggle/working/nltk_data/\")\nnltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\n# Adding this path to nltk so it can observe the files of the packages in it\nnltk.data.path.append(\"/kaggle/working/nltk_data/\")\n\nfrom nltk.corpus import stopwords\n\n# Download English stopwords\nnltk.download('stopwords')\n\n# Load English stopwords\nenglish_stopwords = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:19:04.717967Z","iopub.execute_input":"2024-04-22T09:19:04.718459Z","iopub.status.idle":"2024-04-22T09:22:10.337393Z","shell.execute_reply.started":"2024-04-22T09:19:04.718421Z","shell.execute_reply":"2024-04-22T09:22:10.334293Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2024-04-22 09:19:25--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.51, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: 'cc.en.300.bin.gz'\n\ncc.en.300.bin.gz    100%[===================>]   4.19G  65.5MB/s    in 65s     \n\n2024-04-22 09:20:30 (65.9 MB/s) - 'cc.en.300.bin.gz' saved [4503593528/4503593528]\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data/...\nArchive:  /kaggle/working/nltk_data/corpora/wordnet.zip\n   creating: /kaggle/working/nltk_data/corpora/wordnet/\n  inflating: /kaggle/working/nltk_data/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/README  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/adj.exc  \nArchive:  /kaggle/working/nltk_data/corpora/omw-1.4.zip\n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/\n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/wn-data-fin.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/wn-data-heb.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/wn-data-slv.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/wn-data-ita.tab  \n extracting: /kaggle/working/nltk_data/corpora/omw-1.4/ita/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nno.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nob.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/als/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/wn-data-als.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/wn-data-pol.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/\n extracting: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/wn-data-hrv.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/wn-data-ita.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/wn-data-nld.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/wn-data-ron.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/wn-data-arb.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/wn-data-isl.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/wn-data-swe.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/por/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/wn-data-por.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/wn-data-cmn.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/wn-data-jpn.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/wn-data-dan.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-slk.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-lit.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/wn-data-bul.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-eus.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-cat.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-glg.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-spa.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/wn-data-ell.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-zsm.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-ind.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/wn-data-fra.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/wn-data-tha.tab  \n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Handling Yelp dataset","metadata":{}},{"cell_type":"code","source":"# Handling the Yelp Dataset\ndata_file_path = \"/kaggle/input/yelp_academic_dataset_tip.json\"\ndata_file_name = \"yelp_academic_dataset_tip.json\"\nyelp_datafile = pd.read_json(data_file_path, lines=True)\nprint('List of all columns')\nprint(list(yelp_datafile))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:10.343915Z","iopub.execute_input":"2024-04-22T09:22:10.346975Z","iopub.status.idle":"2024-04-22T09:22:21.816219Z","shell.execute_reply.started":"2024-04-22T09:22:10.346917Z","shell.execute_reply":"2024-04-22T09:22:21.814547Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"List of all columns\n['user_id', 'business_id', 'text', 'date', 'compliment_count']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Taking subset for Gensim model","metadata":{}},{"cell_type":"code","source":"\n# Subset data for gensim fastText model\nall_sentences = list(yelp_datafile['text']) # select \"text\" column only\npart_of_sentences = all_sentences[0:3000] # select the first 3000 sample lines\nprint(\"\\nSamples of Sentences\\n [{}]\".format(part_of_sentences[0:10]))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:21.817840Z","iopub.execute_input":"2024-04-22T09:22:21.818235Z","iopub.status.idle":"2024-04-22T09:22:22.067219Z","shell.execute_reply.started":"2024-04-22T09:22:21.818200Z","shell.execute_reply":"2024-04-22T09:22:22.065225Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nSamples of Sentences\n [['Avengers time with the ladies.', 'They have lots of good deserts and tasty cuban sandwiches', \"It's open even when you think it isn't\", 'Very decent fried chicken', 'Appetizers.. platter special for lunch', 'Chili Cup + Single Cheeseburger with onion, pickle, and relish + Vanilla Coca-Cola...so far.', \"Saturday, Dec 7th 2013, ride Patco's Silver Sleigh w/ Santa & his elves on a decorated train into Center City. Trains leave from Lindenwold at 10am, 11:15am, & 12:30pm, and make all stops. Great for kids!\", 'This is probably the best place in the cool Springs area to watch a game and eat', 'Tacos', 'Starbucks substitute in boring downtown Tampa. Ugh. Never again!']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing the subset we took","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef process_text(document):\n    \n    document = re.sub(r'[^a-zA-Z0-9\\s]', '', document)# Remove non-alphanumeric characters\n    \n    document = re.sub(r'\\w\\d\\w', '', document)# Removing words that have numbers in them\n    \n    document = re.sub(r'[0-9]+', '', document) # Removing digits\n    \n    document = re.sub(r'\\s+', ' ', document, flags=re.I) # Remove extra white space from text\n\n    document = re.sub(r'\\W', ' ', str(document)) # Remove all the special characters from text\n\n    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document) # Remove all single characters from text\n    \n    document = document.lower() # Converting to Lowercase\n    \n    # Word tokenization \n    tokens = document.split()\n    \n    # Applying lemmatization\n    lemma_txt = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Removing stopping words\n    lemma_no_stop_txt = [word for word in lemma_txt if word not in english_stopwords]\n    \n    # Drop words less than 3 characters\n    tokens = [word for word in tokens if len(word) > 3]\n    \n    # Getting unique words\n    indices = np.unique(tokens, return_index=True)[1]\n    cleaned_unique = np.array(tokens)[indices].tolist()\n    \n    return cleaned_unique\ncleaned_reviews = [ process_text(document) for document in part_of_sentences]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:22.070976Z","iopub.execute_input":"2024-04-22T09:22:22.073663Z","iopub.status.idle":"2024-04-22T09:22:25.439584Z","shell.execute_reply.started":"2024-04-22T09:22:22.073587Z","shell.execute_reply":"2024-04-22T09:22:25.437753Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Print the first 15 processed reviews\nfor idx,document in enumerate(cleaned_reviews[0:15]):\n    print(f\"Review {idx+1}: {document}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:25.441346Z","iopub.execute_input":"2024-04-22T09:22:25.441719Z","iopub.status.idle":"2024-04-22T09:22:25.450718Z","shell.execute_reply.started":"2024-04-22T09:22:25.441690Z","shell.execute_reply":"2024-04-22T09:22:25.448807Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Review 1: ['avengers', 'ladies', 'time', 'with']\nReview 2: ['cuban', 'deserts', 'good', 'have', 'lots', 'sandwiches', 'tasty', 'they']\nReview 3: ['even', 'isnt', 'open', 'think', 'when']\nReview 4: ['chicken', 'decent', 'fried', 'very']\nReview 5: ['appetizers', 'lunch', 'platter', 'special']\nReview 6: ['cheeseburger', 'chili', 'cocacolaso', 'onion', 'pickle', 'relish', 'single', 'vanilla', 'with']\nReview 7: ['center', 'city', 'decorated', 'elves', 'from', 'great', 'into', 'kids', 'leave', 'lindenwold', 'make', 'patcos', 'ride', 'santa', 'saturday', 'silver', 'sleigh', 'stops', 'train', 'trains']\nReview 8: ['area', 'best', 'cool', 'game', 'place', 'probably', 'springs', 'this', 'watch']\nReview 9: ['tacos']\nReview 10: ['again', 'boring', 'downtown', 'never', 'starbucks', 'substitute', 'tampa']\nReview 11: ['order', 'soup', 'tortilla']\nReview 12: ['back', 'coming', 'definitely', 'good', 'very', 'will']\nReview 13: ['hotlight', 'must', 'stop']\nReview 14: ['lets', 'yankees']\nReview 15: ['basically', 'food', 'more', 'rallys', 'same']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train our FastText model","metadata":{}},{"cell_type":"code","source":"def train_Fasttext(sentences,embedding_size,window_size,min_word,down_sampling,epochs,Save_model_filename):\n    fast_Text_model = FastText(sentences,\n    vector_size=embedding_size, # Dimensionality of the word vectors. ,\n    window=window_size,\n    min_count=min_word, # The model ignores all words with total frequency lower than this.\n    sample=down_sampling, # threshold which higher-frequency words are randomly down sampled\n    workers = 4, # Num threads to train the model (faster training with multicore comp.)\n    sg=1, # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n    epochs=epochs) # Number of iterations (epochs) over the corpus\n\n    fast_Text_model.save(Save_model_filename) # Save fastText gensim model","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:25.452276Z","iopub.execute_input":"2024-04-22T09:22:25.452644Z","iopub.status.idle":"2024-04-22T09:22:25.465504Z","shell.execute_reply.started":"2024-04-22T09:22:25.452613Z","shell.execute_reply":"2024-04-22T09:22:25.463954Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# selected values for Training parameters\nembedding_size = 500\nwindow_size = 4\nmin_word = 4\ndown_sampling = 1e-2\nepochs=200\n\ntrain_Fasttext(cleaned_reviews,embedding_size,window_size,min_word,down_sampling,epochs,\"Custom_FastText\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:22:25.468552Z","iopub.execute_input":"2024-04-22T09:22:25.469094Z","iopub.status.idle":"2024-04-22T09:23:21.997799Z","shell.execute_reply.started":"2024-04-22T09:22:25.469020Z","shell.execute_reply":"2024-04-22T09:23:21.996560Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load saved gensim fastText model\nfast_Text_model = Word2Vec.load(\"/kaggle/working/Custom_FastText\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:23:21.999760Z","iopub.execute_input":"2024-04-22T09:23:22.000276Z","iopub.status.idle":"2024-04-22T09:23:23.832778Z","shell.execute_reply.started":"2024-04-22T09:23:22.000231Z","shell.execute_reply":"2024-04-22T09:23:23.831189Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Load Pre-trained model","metadata":{}},{"cell_type":"code","source":"# Load pretrained fastText word embeddings\npretrained_fastText_en = load_facebook_model('/kaggle/working/cc.en.300.bin')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:23:23.835112Z","iopub.execute_input":"2024-04-22T09:23:23.835673Z","iopub.status.idle":"2024-04-22T09:26:19.535617Z","shell.execute_reply.started":"2024-04-22T09:23:23.835630Z","shell.execute_reply":"2024-04-22T09:26:19.533978Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Collect word in our model vocabulary","metadata":{}},{"cell_type":"code","source":"words = list(fast_Text_model.wv.key_to_index)  # Collect words from the model's vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:19.540940Z","iopub.execute_input":"2024-04-22T09:26:19.541511Z","iopub.status.idle":"2024-04-22T09:26:19.548221Z","shell.execute_reply.started":"2024-04-22T09:26:19.541471Z","shell.execute_reply":"2024-04-22T09:26:19.546784Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(words)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:19.549657Z","iopub.execute_input":"2024-04-22T09:26:19.550082Z","iopub.status.idle":"2024-04-22T09:26:19.568931Z","shell.execute_reply.started":"2024-04-22T09:26:19.550024Z","shell.execute_reply":"2024-04-22T09:26:19.567188Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['great', 'good', 'with', 'food', 'this', 'they', 'place', 'service', 'best', 'here', 'have', 'very', 'love', 'your', 'time', 'that', 'nice', 'amazing', 'like', 'delicious', 'friendly', 'dont', 'awesome', 'just', 'lunch', 'free', 'their', 'will', 'come', 'menu', 'chicken', 'always', 'only', 'there', 'order', 'from', 'pizza', 'staff', 'back', 'coffee', 'ever', 'breakfast', 'more', 'fresh', 'really', 'beer', 'make', 'open', 'wait', 'some', 'little', 'happy', 'excellent', 'today', 'what', 'after', 'were', 'when', 'everything', 'than', 'hour', 'salad', 'worth', 'sure', 'much', 'price', 'selection', 'pretty', 'people', 'right', 'closed', 'well', 'better', 'atmosphere', 'parking', 'sushi', 'cheese', 'fried', 'about', 'yummy', 'been', 'check', 'soup', 'even', 'night', 'down', 'want', 'early', 'location', 'favorite', 'shrimp', 'specials', 'restaurant', 'work', 'take', 'must', 'dinner', 'cant', 'drink', 'before', 'wings', 'perfect', 'other', 'area', 'special', 'drinks', 'wine', 'around', 'burgers', 'sweet', 'call', 'cool', 'youre', 'fries', 'them', 'never', 'sandwich', 'still', 'first', 'lots', 'every', 'quick', 'need', 'store', 'again', 'spot', 'bring', 'prices', 'town', 'tasty', 'busy', 'customer', 'music', 'getting', 'minutes', 'know', 'slow', 'also', 'until', 'made', 'clean', 'business', 'small', 'because', 'sunday', 'tacos', 'local', 'experience', 'burger', 'next', 'nothing', 'kids', 'which', 'super', 'last', 'rice', 'half', 'brunch', 'chocolate', 'patio', 'fish', 'hours', 'yelp', 'side', 'live', 'looks', 'outside', 'sauce', 'long', 'ordered', 'week', 'green', 'thats', 'spicy', 'options', 'over', 'anything', 'monday', 'stop', 'would', 'family', 'huge', 'coming', 'same', 'roll', 'going', 'park', 'close', 'beautiful', 'didnt', 'where', 'cold', 'rolls', 'steak', 'server', 'while', 'soon', 'full', 'available', 'white', 'years', 'plus', 'look', 'morning', 'chips', 'taste', 'tonight', 'came', 'bread', 'items', 'most', 'cream', 'though', 'beef', 'home', 'fast', 'weekend', 'sundays', 'room', 'shop', 'sign', 'street', 'oysters', 'burrito', 'inside', 'find', 'large', 'theyre', 'quality', 'birthday', 'theres', 'cheap', 'could', 'recommend', 'gets', 'line', 'miss', 'else', 'seafood', 'then', 'plenty', 'many', 'late', 'game', 'pork', 'through', 'makes', 'table', 'water', 'tuesday', 'cash', 'during', 'enough', 'later', 'another', 'seating', 'hard', 'fantastic', 'visit', 'potato', 'dessert', 'went', 'waiting', 'watch', 'thanks', 'helpful', 'house', 'thing', 'regular', 'eating', 'meal', 'sandwiches', 'authentic', 'phone', 'think', 'said', 'away', 'italian', 'seems', 'flavors', 'cake', 'owner', 'show', 'taco', 'wonderful', 'saturday', 'wifi', 'owners', 'buffet', 'looking', 'walk', 'baked', 'doing', 'keep', 'hands', 'disappointed', 'feel', 'spring', 'under', 'drive', 'manager', 'called', 'style', 'crazy', 'wont', 'expensive', 'told', 'meat', 'onion', 'maybe', 'cute', 'party', 'summer', 'enjoy', 'beans', 'door', 'these', 'highly', 'friday', 'help', 'trying', 'card', 'start', 'dining', 'wrong', 'terrible', 'times', 'both', 'dumplings', 'less', 'mondays', 'wednesday', 'avoid', 'wasnt', 'money', 'crispy', 'ahead', 'worst', 'healthy', 'city', 'into', 'kitchen', 'wish', 'meeting', 'choice', 'give', 'tastes', 'should', 'extremely', 'bacon', 'review', 'corn', 'reservation', 'frozen', 'pool', 'starbucks', 'downtown', 'probably', 'okay', 'prepared', 'chili', 'decent', 'cuban', 'grilled', 'person', 'longer', 'found', 'done', 'homemade', 'eggs', 'something', 'beers', 'charge', 'black', 'especially', 'milk', 'flavor', 'veggie', 'crowd', 'market', 'variety', 'having', 'west', 'used', 'event', 'yourself', 'dogs', 'lobster', 'deal', 'turkey', 'serve', 'pepper', 'duck', 'short', 'cards', 'rude', 'curry', 'thursday', 'delivery', 'please', 'leave', 'each', 'horrible', 'does', 'taking', 'crab', 'poor', 'youll', 'between', 'bomb', 'salty', 'cocktails', 'thai', 'portion', 'employees', 'extra', 'loud', 'everyone', 'working', 'waiter', 'outdoor', 'tampa', 'ready', 'friends', 'needs', 'ladies', 'instead', 'post', 'french', 'appetizer', 'byob', 'finally', 'waitress', 'margaritas', 'things', 'lovely', 'weekends', 'crowded', 'orders', 'limited', 'being', 'appointment', 'blueberry', 'walking', 'credit', 'yogurt', 'philly', 'across', 'tour', 'doesnt', 'outstanding', 'lines', 'days', 'incredible', 'cafe', 'three', 'light', 'slice', 'holiday', 'average', 'delish', 'whole', 'offer', 'nashville', 'portions', 'value', 'world', 'took', 'dirty', 'such', 'pickles', 'comfortable', 'glass', 'chinese', 'shot', 'mango', 'definitely', 'plate', 'save', 'office', 'high', 'month', 'brew', 'floor', 'year', 'vegan', 'front', 'since', 'salsa', 'afternoon', 'interesting', 'strawberry', 'catch', 'usually', 'lets', 'option', 'takes', 'checked', 'bathroom', 'tomato', 'stuff', 'different', 'forget', 'says', 'actually', 'sell', 'nights', 'empty', 'expect', 'once', 'absolutely', 'move', 'cooked', 'building', 'choose', 'ribs', 'isnt', 'roasted', 'veggies', 'waited', 'salads', 'treat', 'honey', 'tried', 'double', 'salmon', 'thank', 'unique', 'smells', 'online', 'real', 'comes', 'margarita', 'pancakes', 'bloody', 'fabulous', 'anywhere', 'foods', 'tasting', 'together', 'classic', 'changes', 'espresso', 'thursdays', 'iced', 'mini', 'fair', 'dish', 'sales', 'flavorful', 'least', 'kind', 'located', 'yeah', 'garden', 'nail', 'peppers', 'owned', 'waffles', 'heaven', 'impressed', 'tofu', 'skip', 'serving', 'bottles', 'sliders', 'draft', 'almost', 'ambiance', 'diner', 'toast', 'website', 'filet', 'sides', 'chai', 'clear', 'warm', 'easy', 'overpriced', 'issues', 'generous', 'checkin', 'bartender', 'burritos', 'affordable', 'apple', 'however', 'bathrooms', 'appetizers', 'reservations', 'club', 'ginger', 'type', 'class', 'able', 'opinion', 'grab', 'without', 'quite', 'daily', 'might', 'recommended', 'places', 'roast', 'pasta', 'tell', 'space', 'stars', 'gift', 'entree', 'cozy', 'section', 'croissants', 'couldnt', 'awful', 'milkshakes', 'bisque', 'chef', 'meats', 'hole', 'wall', 'season', 'consistent', 'advance', 'pick', 'cheesesteak', 'wouldnt', 'served', 'along', 'plain', 'chance', 'latte', 'eaten', 'hummus', 'packed', 'fill', 'dishes', 'spinach', 'heard', 'believe', 'stay', 'hoagies', 'evening', 'feeling', 'rush', 'state', 'irish', 'bites', 'list', 'college', 'blue', 'using', 'tuesdays', 'exactly', 'self', 'several', 'mary', 'juice', 'arrive', 'locally', 'baby', 'football', 'beat', 'catfish', 'sample', 'typical', 'vanilla', 'killer', 'date', 'center', 'pricey', 'wanted', 'chicago', 'massage', 'mimosas', 'board', 'snack', 'cleaning', 'already', 'school', 'dude', 'sitting', 'asked', 'rock', 'alcohol', 'nasty', 'truffle', 'months', 'tuna', 'mexican', 'wednesdays', 'bakery', 'view', 'cakes', 'gotta', 'shes', 'life', 'pleasant', 'waste', 'window', 'dancing', 'guys', 'alternative', 'closing', 'near', 'counter', 'currently', 'girl', 'theyll', 'christmas', 'brand', 'care', 'lobby', 'offering', 'sucks', 'chip', 'seen', 'games', 'four', 'opening', 'garlic', 'matter', 'fall', 'sangria', 'reasonable', 'shopping', 'wrap', 'orange', 'size', 'second', 'vegetarian', 'stopped', 'round', 'youve', 'views', 'coconut', 'stick', 'seriously', 'checking', 'behind', 'moved', 'wife', 'bowl', 'organized', 'venue', 'accommodating', 'nachos', 'gelato', 'normally', 'spaghetti', 'totally', 'machine', 'popcorn', 'unfortunately', 'mins', 'station', 'rather', 'forward', 'indy', 'banana', 'sports', 'prime', 'incredibly', 'minute', 'cups', 'bartenders', 'pete', 'priced', 'reasonably', 'reviews', 'cook', 'lettuce', 'idea', 'dark', 'lost', 'quiet', 'number', 'groceries', 'hope', 'seats', 'single', 'note', 'relish', 'arrived', 'pass', 'tasted', 'running', 'cashier', 'butter', 'platter', 'protein', 'liked', 'cheeses', 'rooms', 'locals', 'smooth', 'simple', 'filling', 'neighborhood', 'hurry', 'falafel', 'words', 'louis', 'blackberry', 'bucks', 'products', 'cajun', 'split', 'playing', 'play', 'vodka', 'bottle', 'true', 'part', 'friend', 'smelled', 'weeks', 'warning', 'unless', 'head', 'stout', 'asian', 'offered', 'guacamole', 'lasagna', 'velvet', 'bottomless', 'seasoned', 'foot', 'dollar', 'movie', 'creme', 'taken', 'purchase', 'return', 'changed', 'charbroiled', 'sugar', 'discount', 'boba', 'lucky', 'menus', 'beware', 'caramel', 'kinda', 'trust', 'tables', 'mind', 'started', 'entire', 'valet', 'donut', 'courtyard', 'left', 'forever', 'coke', 'clothes', 'sister', 'deli', 'seemed', 'craft', 'plan', 'ends', 'enjoyed', 'hair', 'waffle', 'tomorrow', 'mmmm', 'king', 'book', 'bite', 'stale', 'guess', 'gone', 'pack', 'rings', 'name', 'address', 'tender', 'heres', 'gave', 'hate', 'karaoke', 'bean', 'cocktail', 'saturdays', 'meatballs', 'knows', 'sale', 'fruit', 'mediocre', 'soda', 'chipotle', 'insane', 'shoe', 'anyone', 'biscuits', 'possibly', 'bank', 'ownership', 'attentive', 'raining', 'seat', 'strange', 'mark', 'solid', 'face', 'gluten', 'sunny', 'hand', 'cider', 'buffalo', 'american', 'fine', 'services', 'above', 'pickup', 'coupon', 'bowling', 'simply', 'cost', 'write', 'beignets', 'secret']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Printing some random words results to see models performance","metadata":{}},{"cell_type":"code","source":"import random\n\n# Define a function to find top similar and dissimilar words\ndef find_top_n(word, word_list, model, n=10):\n    similarities = model.wv.most_similar(word, topn=n)\n    return similarities\n\n# Loop to process words\nfor word in random.sample(words, 20):  # Take 20 random words from 'words' list\n    print(f\"Analyzing word: {word}\\n\")  # Print the word being analyzed\n\n    # Custom Model\n    similar_custom = find_top_n(word, words, fast_Text_model)\n    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n    \n    # Pretrained Model\n    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n    \n    # Print the output\n    print(\"Top 10 similar words (custom model):\")\n    for similar_word, similarity in similar_custom:\n        print(f\"{similar_word}: {similarity:.4f}\")\n    \n    print(\"\\nTop 10 opposite words (custom model):\")\n    for opposite_word, similarity in dissimilar_custom:\n        print(f\"{opposite_word}: {similarity:.4f}\")\n\n    print(\"\\nTop 10 similar words (pre-trained model):\")\n    for similar_word, similarity in similar_pretrained:\n        print(f\"{similar_word}: {similarity:.4f}\")\n\n    print(\"\\nTop 10 opposite words (pre-trained model):\")\n    for opposite_word, similarity in dissimilar_pretrained:\n        print(f\"{opposite_word}: {similarity:.4f}\")\n    \n    print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:19.572229Z","iopub.execute_input":"2024-04-22T09:26:19.572788Z","iopub.status.idle":"2024-04-22T09:26:29.666150Z","shell.execute_reply.started":"2024-04-22T09:26:19.572719Z","shell.execute_reply":"2024-04-22T09:26:29.663287Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Analyzing word: behind\n\nTop 10 similar words (custom model):\nbeignets: 0.5661\ndude: 0.5193\ncashier: 0.5035\nalcohol: 0.4847\nbeware: 0.4794\nmind: 0.4752\nfind: 0.4688\nyoull: 0.4672\nbrand: 0.4671\nbathroom: 0.4584\n\nTop 10 opposite words (custom model):\norder: 0.0264\nvariety: 0.0250\nlunch: 0.0181\nsuch: 0.0106\nwith: 0.0106\nfresh: 0.0025\ntogether: 0.0001\nsliders: -0.0033\nyogurt: -0.0053\nspecials: -0.0079\n\nTop 10 similar words (pre-trained model):\nbehing: 0.7815\nbeind: 0.6468\nBehind: 0.6307\nbehin: 0.6221\nbehnd: 0.6189\nbehid: 0.6137\nbehindthe: 0.6007\nbehind.The: 0.5871\nBEHIND: 0.5320\nbehi: 0.5294\n\nTop 10 opposite words (pre-trained model):\nswept: 0.2916\nclearing: 0.2916\nlinger: 0.2916\nhouse.Just: 0.2915\ndown.There: 0.2915\ndown.They: 0.2915\naway.With: 0.2915\narout: 0.2915\nforward.At: 0.2915\nstymied: 0.2915\n\n----------------------------------------\n\nAnalyzing word: even\n\nTop 10 similar words (custom model):\nevent: 0.5197\nevening: 0.4565\nheaven: 0.3800\ncajun: 0.3649\nclosing: 0.3604\nadvance: 0.3543\nsince: 0.3534\nbeat: 0.3478\ncrowded: 0.3387\nattentive: 0.3384\n\nTop 10 opposite words (custom model):\nbuilding: 0.0182\ndishes: 0.0090\nmust: 0.0088\nspring: 0.0072\nsalad: 0.0018\nwest: -0.0012\nperfect: -0.0280\nveggies: -0.0290\nunique: -0.0337\nyoure: -0.0351\n\nTop 10 similar words (pre-trained model):\nthough: 0.7639\nEven: 0.7537\nbut: 0.6741\nhardly: 0.6741\nmoreso: 0.6550\nactually: 0.6412\nperhaps: 0.6333\nwouldn: 0.6244\npossibly: 0.6233\nBut: 0.6213\n\nTop 10 opposite words (pre-trained model):\nmuch.And: 0.4234\nbefore.If: 0.4234\nwould.You: 0.4234\nimagined.It: 0.4233\nundeniably: 0.4232\nhas.And: 0.4232\nmore.Maybe: 0.4232\nceratinly: 0.4232\njustifiably: 0.4232\ndoubted: 0.4231\n\n----------------------------------------\n\nAnalyzing word: shop\n\nTop 10 similar words (custom model):\nshopping: 0.5975\nshoe: 0.5423\nshot: 0.5069\nshow: 0.4701\nsecret: 0.4549\nhowever: 0.4269\nshort: 0.4119\nginger: 0.4103\nshould: 0.3978\nhope: 0.3966\n\nTop 10 opposite words (custom model):\nmediocre: 0.0351\nwhen: 0.0333\nthat: 0.0309\noffered: 0.0306\nsave: 0.0235\nchecked: 0.0189\nfamily: 0.0129\ncash: 0.0020\nfull: -0.0251\nwere: -0.0362\n\nTop 10 similar words (pre-trained model):\nshops: 0.7265\nshop.Now: 0.7217\nshop.When: 0.7124\nshop.We: 0.7104\nshop.But: 0.7057\nshop.: 0.7037\nshop.I: 0.7005\nshop.This: 0.6928\nshop.It: 0.6910\nstore: 0.6897\n\nTop 10 opposite words (pre-trained model):\nbuy.A: 0.3756\nboutique-y: 0.3756\njob-lot: 0.3756\ndry-cleaning: 0.3755\nbaristas: 0.3755\ncoffeshops: 0.3755\ncabinet-making: 0.3754\ncar-boot: 0.3754\nbeautician: 0.3753\ncaffe: 0.3752\n\n----------------------------------------\n\nAnalyzing word: year\n\nTop 10 similar words (custom model):\nyears: 0.6844\nyeah: 0.6470\nbaby: 0.4652\nbloody: 0.4641\nwaste: 0.4509\nnear: 0.4487\nwithout: 0.4455\nwall: 0.4454\ntonight: 0.4411\nwould: 0.4192\n\nTop 10 opposite words (custom model):\ndeal: 0.0388\naverage: 0.0323\nstale: 0.0230\nnice: 0.0221\nitalian: 0.0100\nexperience: 0.0061\nfrom: -0.0011\ndont: -0.0206\nhuge: -0.0244\nmeal: -0.0316\n\nTop 10 similar words (pre-trained model):\nmonth: 0.7769\nyear.This: 0.7441\nyear.Last: 0.6821\nweek: 0.6800\nyear.Now: 0.6705\nyear-and: 0.6699\nyear.The: 0.6682\nyear.So: 0.6660\nlast: 0.6658\nmonths: 0.6638\n\nTop 10 opposite words (pre-trained model):\nover.Last: 0.4304\nLast: 0.4304\nweek.Is: 0.4303\nmid-december: 0.4303\nNov-Jan: 0.4303\nyear-end: 0.4303\nannualy: 0.4303\nyear-one: 0.4301\n2013.I: 0.4300\nmonths.What: 0.4298\n\n----------------------------------------\n\nAnalyzing word: shopping\n\nTop 10 similar words (custom model):\nshop: 0.5975\ngirl: 0.5434\nbaby: 0.5396\nvalet: 0.5151\nshoe: 0.4764\nshow: 0.4660\ncenter: 0.4635\ntrying: 0.4620\ntypical: 0.4578\nlocated: 0.4470\n\nTop 10 opposite words (custom model):\ncash: 0.0505\ncajun: 0.0478\nbeer: 0.0413\nenjoyed: 0.0406\nfish: 0.0400\nblueberry: 0.0332\nchips: 0.0244\nprices: 0.0241\ngood: -0.0007\ntastes: -0.0296\n\nTop 10 similar words (pre-trained model):\nshoppng: 0.7567\nshoppping: 0.7366\nShopping: 0.7202\nshopping.The: 0.6853\nshopping-: 0.6778\nnon-shopping: 0.6778\nshoping: 0.6750\npre-shopping: 0.6739\nshopping.It: 0.6728\nshopping.I: 0.6719\n\nTop 10 opposite words (pre-trained model):\nshowroom: 0.3384\nstors: 0.3384\nchristmas: 0.3384\nbike-riding: 0.3383\nshoped: 0.3383\nTJMaxx: 0.3382\nresterants: 0.3382\nTheFind: 0.3382\negg-hunting: 0.3381\nMaceys: 0.3381\n\n----------------------------------------\n\nAnalyzing word: take\n\nTop 10 similar words (custom model):\ntaken: 0.5218\ntakes: 0.4895\nwatch: 0.4277\ntaking: 0.4120\nahead: 0.3860\nkaraoke: 0.3757\ncake: 0.3540\nwife: 0.3469\nladies: 0.3392\nimpressed: 0.3371\n\nTop 10 opposite words (custom model):\nmaybe: 0.0077\nflavor: 0.0015\nenjoyed: 0.0014\ndisappointed: -0.0017\ngood: -0.0037\nhealthy: -0.0122\ngreen: -0.0363\nchicken: -0.0399\nbetter: -0.0514\nitalian: -0.0541\n\nTop 10 similar words (pre-trained model):\ntaking: 0.7341\ntakes: 0.7027\ntook: 0.6750\nTake: 0.6565\ntaken: 0.6423\nTaking: 0.6378\ngive: 0.6092\nget: 0.5925\ntkae: 0.5773\nput: 0.5694\n\nTop 10 opposite words (pre-trained model):\nit.Going: 0.3433\nproceeed: 0.3433\nre-consider: 0.3432\nelevate: 0.3432\nwait.If: 0.3432\nfurther: 0.3431\nabdicate: 0.3431\nbe.First: 0.3430\nlittle: 0.3430\nseriously.Click: 0.3430\n\n----------------------------------------\n\nAnalyzing word: beans\n\nTop 10 similar words (custom model):\nbean: 0.7413\ncoconut: 0.5511\nbowl: 0.5503\nfilet: 0.5026\ncatfish: 0.4849\nbeat: 0.4812\nbakery: 0.4780\nbottomless: 0.4744\ncaramel: 0.4662\ndonut: 0.4518\n\nTop 10 opposite words (custom model):\nrestaurant: 0.0399\ngive: 0.0395\nmenu: 0.0384\nduring: 0.0367\nminute: 0.0345\ninteresting: 0.0308\nsister: 0.0276\nstopped: 0.0105\nmonth: 0.0087\nstop: 0.0057\n\nTop 10 similar words (pre-trained model):\nbean: 0.7751\nbeans.: 0.7676\nbeans-: 0.7640\nbeans.I: 0.7506\nbeans.The: 0.7501\ngarbonzo: 0.6656\ncanellini: 0.6656\nBeans: 0.6538\nlentils: 0.6488\npeas: 0.6444\n\nTop 10 opposite words (pre-trained model):\negg-plant: 0.3717\nflaxseeds: 0.3717\nhummos: 0.3716\nchile.: 0.3716\nshallots: 0.3716\npepperocini: 0.3715\nchaufa: 0.3715\nomlettes: 0.3714\npresoaking: 0.3714\nbabyfood: 0.3713\n\n----------------------------------------\n\nAnalyzing word: post\n\nTop 10 similar words (custom model):\nlost: 0.5879\ncost: 0.5479\neveryone: 0.4579\nmins: 0.4390\nhurry: 0.4288\npossibly: 0.4223\ngone: 0.4188\nmost: 0.4150\nrather: 0.4050\ncrab: 0.3951\n\nTop 10 opposite words (custom model):\nbartender: 0.0140\nawful: 0.0133\ngetting: 0.0110\ncool: 0.0101\nbeautiful: 0.0097\nhands: 0.0059\nfamily: -0.0007\ndouble: -0.0095\nburrito: -0.0115\ngarlic: -0.0419\n\nTop 10 similar words (pre-trained model):\nblog: 0.7147\nposts: 0.7059\nblogpost: 0.6880\nposting: 0.6699\npost.So: 0.6555\npost.This: 0.6518\npost.Now: 0.6501\npost.And: 0.6415\npost.I: 0.6341\npost.The: 0.6308\n\nTop 10 opposite words (pre-trained model):\npost.my: 0.3860\nbloggerville: 0.3860\nit.p.s.: 0.3858\nblogs.I: 0.3858\nhaloscan: 0.3857\nscreen-grab: 0.3857\nplace.Anyway: 0.3857\nview.Thanks: 0.3856\npictres: 0.3856\narticle.However: 0.3856\n\n----------------------------------------\n\nAnalyzing word: locals\n\nTop 10 similar words (custom model):\nlocal: 0.6717\nlocally: 0.6346\ntuna: 0.5140\nkinda: 0.4850\ncooked: 0.4818\nneighborhood: 0.4758\nprime: 0.4635\ncourtyard: 0.4626\nlocation: 0.4584\nfabulous: 0.4561\n\nTop 10 opposite words (custom model):\nhave: 0.0539\nagain: 0.0524\nappetizer: 0.0509\never: 0.0448\nthree: 0.0440\nwith: 0.0426\nstay: 0.0379\nworld: 0.0268\nbucks: 0.0222\nbelieve: 0.0160\n\nTop 10 similar words (pre-trained model):\nnon-locals: 0.7628\ntourists: 0.7420\nLocals: 0.7404\nout-of-towners: 0.6769\nex-pats: 0.6638\nlocals.The: 0.6584\nlocals-: 0.6539\nexpats: 0.6193\ndaytrippers: 0.6173\nforeigners: 0.6065\n\nTop 10 opposite words (pre-trained model):\ntown-based: 0.3780\nMobilians: 0.3779\nBrazillians: 0.3779\nwoodsmen: 0.3777\nriders: 0.3777\npick-pockets: 0.3776\nFestival-goers: 0.3776\ncosmopolites: 0.3776\nexpeditioners: 0.3776\nlowlifes: 0.3776\n\n----------------------------------------\n\nAnalyzing word: miss\n\nTop 10 similar words (custom model):\ncrab: 0.4397\npopcorn: 0.4373\ntype: 0.4190\ntruffle: 0.4120\ncraft: 0.4085\npass: 0.4015\nglass: 0.4007\nlimited: 0.3845\nhair: 0.3753\ntomorrow: 0.3678\n\nTop 10 opposite words (custom model):\nspinach: 0.0257\nperfect: 0.0257\nbaked: 0.0240\nevery: 0.0225\nbrew: 0.0194\nfrozen: 0.0123\nbefore: 0.0094\nirish: 0.0077\ncoffee: -0.0175\nready: -0.0178\n\nTop 10 similar words (pre-trained model):\nmissed: 0.7433\nforget: 0.6227\nmisssed: 0.5641\nmiss.I: 0.5526\nmiss-out: 0.5513\nmisses: 0.5423\nfoget: 0.5353\nMissed: 0.5306\nmissed.I: 0.5172\ndearly: 0.5126\n\nTop 10 opposite words (pre-trained model):\nknwo: 0.3256\nhellloooo: 0.3255\nsoooooooooooooooooo: 0.3255\nlaugh.You: 0.3254\nmuch.Today: 0.3254\ndarnit: 0.3254\nuldn: 0.3254\nhappent: 0.3253\nloveeeee: 0.3253\nem.I: 0.3253\n\n----------------------------------------\n\nAnalyzing word: coupon\n\nTop 10 similar words (custom model):\ndollar: 0.6079\ncounter: 0.5959\nmachine: 0.5278\norange: 0.5045\nstrange: 0.4862\nwrap: 0.4792\nsave: 0.4724\nchipotle: 0.4685\nlife: 0.4677\nsangria: 0.4604\n\nTop 10 opposite words (custom model):\nlove: 0.0678\nfriendly: 0.0672\nfavorite: 0.0562\nbaked: 0.0503\namazing: 0.0451\ngreat: 0.0378\ngood: 0.0373\nyummy: 0.0360\nbest: 0.0225\nplace: -0.0052\n\nTop 10 similar words (pre-trained model):\ncoupons: 0.8220\ncoupn: 0.7605\nCoupon: 0.7349\ne-coupon: 0.7185\necoupon: 0.7004\ncoupon.: 0.7001\ndiscount: 0.6848\nBOGO: 0.6715\nCOUPON: 0.6710\nB1G1F: 0.6648\n\nTop 10 opposite words (pre-trained model):\nStableHost: 0.3656\nFabric.com: 0.3655\nCoffee-Mate: 0.3654\nkarmaloop: 0.3654\nUlta.com: 0.3654\n20pk: 0.3654\nJCPenny: 0.3654\nBabyoye: 0.3653\nTJMAXX: 0.3653\nmytheresa: 0.3653\n\n----------------------------------------\n\nAnalyzing word: strawberry\n\nTop 10 similar words (custom model):\nblackberry: 0.6825\nmilkshakes: 0.5711\ncurry: 0.5237\ncaramel: 0.5195\nstout: 0.5119\nmilk: 0.5079\nvanilla: 0.5074\nchip: 0.5043\nblueberry: 0.5009\nstick: 0.4953\n\nTop 10 opposite words (custom model):\nafter: 0.0420\nbeware: 0.0417\nbaby: 0.0401\nsunday: 0.0399\nmediocre: 0.0396\nuntil: 0.0363\nbest: 0.0259\ndaily: 0.0247\nreturn: -0.0026\nthank: -0.0104\n\nTop 10 similar words (pre-trained model):\nblueberry: 0.8134\nstawberry: 0.7970\nraspberry: 0.7821\nstrawberries: 0.7627\nberry: 0.7378\npeach: 0.7070\nstrawberrys: 0.6932\nstrawbery: 0.6896\nbluberry: 0.6857\nstrawberry-: 0.6783\n\nTop 10 opposite words (pre-trained model):\ncoconut-milk: 0.4361\nfriut: 0.4361\nflowers.This: 0.4361\nover-ripened: 0.4361\npies: 0.4361\nvine-ripe: 0.4360\neclairs: 0.4360\nlavender-pink: 0.4360\njam: 0.4360\nmini-cupcakes: 0.4359\n\n----------------------------------------\n\nAnalyzing word: tastes\n\nTop 10 similar words (custom model):\ntasted: 0.7066\ntaste: 0.6750\nguacamole: 0.4943\ntasting: 0.4489\nchristmas: 0.4306\nbrew: 0.4088\ncaramel: 0.4071\nlobby: 0.4029\ntasty: 0.3987\nbeans: 0.3900\n\nTop 10 opposite words (custom model):\nhard: 0.0146\nthere: 0.0123\nonce: 0.0100\nmondays: 0.0067\nanother: 0.0054\nlunch: 0.0000\nlonger: -0.0108\nshopping: -0.0296\nstreet: -0.0320\nfirst: -0.0646\n\nTop 10 similar words (pre-trained model):\ntaste: 0.7609\ntastes-: 0.6863\ntasts: 0.6680\ntastes.: 0.6582\ntastes.The: 0.6540\ntastes.I: 0.6310\nTastes: 0.6284\nlikings: 0.6080\ndistastes: 0.5917\ntaste.It: 0.5855\n\nTop 10 opposite words (pre-trained model):\nTasted: 0.3265\nTASTE: 0.3264\n.different: 0.3264\nnon-cooked: 0.3264\nstew: 0.3264\nno-cal: 0.3263\nliquidy: 0.3263\nneeds.But: 0.3262\nprice-range: 0.3262\nphilistine: 0.3262\n\n----------------------------------------\n\nAnalyzing word: crazy\n\nTop 10 similar words (custom model):\ncrab: 0.4669\nnachos: 0.4441\nbeers: 0.4293\nalmost: 0.4292\nbucks: 0.4237\ndessert: 0.4176\nvelvet: 0.3889\nsucks: 0.3884\ncomes: 0.3873\nends: 0.3863\n\nTop 10 opposite words (custom model):\ncouldnt: 0.0355\nyummy: 0.0331\nthank: 0.0307\nreview: 0.0201\nspecial: 0.0188\nprobably: 0.0121\nwouldnt: 0.0040\nthings: 0.0021\nwere: 0.0020\nmade: -0.0238\n\nTop 10 similar words (pre-trained model):\ninsane: 0.7882\nnutso: 0.7844\ncraaaazy: 0.7491\ncraaaaazy: 0.7411\ncraaaaaazy: 0.7272\nmad: 0.7190\ncrazier: 0.7091\nsuper-crazy: 0.7071\nCRAZY: 0.7000\nwhacky: 0.6997\n\nTop 10 opposite words (pre-trained model):\nbaffling: 0.4086\ngoddamned: 0.4085\ndaggone: 0.4085\nbeeotch: 0.4085\nnonesense: 0.4084\nhecka: 0.4084\ndumb-assed: 0.4084\nlonggggg: 0.4084\nSOOOOOO: 0.4084\nsounds: 0.4083\n\n----------------------------------------\n\nAnalyzing word: than\n\nTop 10 similar words (custom model):\nthank: 0.4743\nthanks: 0.4351\nmachine: 0.4238\nsitting: 0.3909\nwalking: 0.3769\nwanted: 0.3742\nfine: 0.3708\nsimply: 0.3664\nthings: 0.3555\nyogurt: 0.3503\n\nTop 10 opposite words (custom model):\nfall: 0.0101\npork: 0.0059\ncurrently: 0.0008\nhere: -0.0019\ndraft: -0.0044\nfried: -0.0087\nbathroom: -0.0107\nhalf: -0.0124\nappetizers: -0.0218\nchicken: -0.0750\n\nTop 10 similar words (pre-trained model):\nless: 0.7572\nthatn: 0.7450\nmore: 0.6799\nthann: 0.6765\nthant: 0.6719\nfewer: 0.6296\nrather: 0.6158\nthasn: 0.5994\ntahn: 0.5940\nbetter: 0.5912\n\nTop 10 opposite words (pre-trained model):\nimagined-: 0.3682\nheaver: 0.3682\npoofier: 0.3681\nspringier: 0.3680\ngorier: 0.3680\nbetter-funded: 0.3680\nlikel: 0.3677\nlonger.If: 0.3676\nsmalller: 0.3675\nwhile: 0.3675\n\n----------------------------------------\n\nAnalyzing word: offer\n\nTop 10 similar words (custom model):\noffered: 0.6682\noffering: 0.6572\noffice: 0.4881\nonline: 0.4778\nmonth: 0.4768\nlonger: 0.4587\ncashier: 0.4354\nsecond: 0.4248\nmins: 0.4209\nlost: 0.4157\n\nTop 10 opposite words (custom model):\namazing: 0.0286\ndidnt: 0.0281\nenjoy: 0.0249\nwouldnt: 0.0233\ntell: 0.0214\ndrink: 0.0163\nwhat: 0.0096\ndecent: 0.0070\nbeautiful: -0.0069\ncant: -0.0315\n\nTop 10 similar words (pre-trained model):\noffers: 0.7695\noffering: 0.7514\noffered: 0.7177\nprovide: 0.6549\noffer.They: 0.5938\noffer.So: 0.5754\noffer.As: 0.5736\noffer.To: 0.5702\noffereing: 0.5655\noffere: 0.5654\n\nTop 10 opposite words (pre-trained model):\nHappySale: 0.3263\none-of-a-: 0.3263\navailabe: 0.3263\n29Make: 0.3263\ncompete: 0.3262\ncould: 0.3262\nsupport.There: 0.3261\nsatisfactory: 0.3260\nandreceive: 0.3259\nunconditional: 0.3259\n\n----------------------------------------\n\nAnalyzing word: currently\n\nTop 10 similar words (custom model):\ndonut: 0.5826\nsoda: 0.5676\ncheckin: 0.5212\npacked: 0.5183\nrunning: 0.5009\nthree: 0.4937\nexpect: 0.4907\nholiday: 0.4829\nrelish: 0.4629\nexactly: 0.4608\n\nTop 10 opposite words (custom model):\nbest: 0.0481\nwith: 0.0459\nsmall: 0.0308\nquick: 0.0270\nimpressed: 0.0193\nrice: 0.0088\nthan: 0.0008\nthough: 0.0005\nwont: -0.0021\norder: -0.0100\n\nTop 10 similar words (pre-trained model):\ncurrenly: 0.7646\npresently: 0.7569\ncurrenlty: 0.7524\ncurrrently: 0.7317\nCurrently: 0.7228\ncurrenty: 0.6689\ncurently: 0.6386\ncurrentl: 0.5907\nstill: 0.5796\ncurretly: 0.5760\n\nTop 10 opposite words (pre-trained model):\nfor.I: 0.2661\nre-locating: 0.2661\nknows: 0.2660\npost-holder: 0.2660\npreferred.This: 0.2660\ntoday.While: 0.2660\nperforms: 0.2660\nalreaady: 0.2659\n2010-2012: 0.2658\navailableIf: 0.2658\n\n----------------------------------------\n\nAnalyzing word: hurry\n\nTop 10 similar words (custom model):\nclosing: 0.5350\ncurry: 0.5160\nleast: 0.5027\ngone: 0.4817\nblackberry: 0.4749\nchef: 0.4741\nbeignets: 0.4568\nforget: 0.4539\nclose: 0.4504\nalcohol: 0.4474\n\nTop 10 opposite words (custom model):\nsides: 0.0507\nsports: 0.0497\nparty: 0.0454\nneed: 0.0442\npasta: 0.0432\nmakes: 0.0416\nside: 0.0292\nowner: 0.0170\npizza: -0.0176\nsweet: -0.0281\n\nTop 10 similar words (pre-trained model):\nhurrying: 0.6769\nrush: 0.6202\nhurry.I: 0.6155\nHurry: 0.6121\nhurried: 0.6094\nhurry-: 0.6021\nhurry.: 0.5910\ndilly-dally: 0.5863\ndawdle: 0.5576\nhurry.The: 0.5437\n\nTop 10 opposite words (pre-trained model):\nthere.Don: 0.3237\nmomentto: 0.3237\nEARLY: 0.3236\nneeeeed: 0.3236\naboutto: 0.3236\na-goin: 0.3235\nreadied: 0.3235\npicked: 0.3234\ncheck: 0.3234\nnooooooo: 0.3234\n\n----------------------------------------\n\nAnalyzing word: thats\n\nTop 10 similar words (custom model):\nmeats: 0.4664\nseats: 0.4394\nthat: 0.4029\npancakes: 0.3909\ntaken: 0.3714\nthai: 0.3695\nmeatballs: 0.3646\nexactly: 0.3615\nkaraoke: 0.3560\nweeks: 0.3510\n\nTop 10 opposite words (custom model):\nrolls: 0.0174\nbest: 0.0162\nbacon: 0.0147\nmenu: 0.0100\nplatter: 0.0080\neveryone: 0.0060\nfloor: 0.0045\nabout: -0.0042\nalways: -0.0194\nfree: -0.0266\n\nTop 10 similar words (pre-trained model):\nThats: 0.8654\nisnt: 0.8247\nwhats: 0.7842\ntheres: 0.7775\ndont: 0.7758\ntahts: 0.7601\nshouldnt: 0.7384\njsut: 0.7367\ngeuss: 0.7325\nbasicly: 0.7293\n\nTop 10 opposite words (pre-trained model):\nhink: 0.5489\nme.well: 0.5489\nsomehing: 0.5488\nhaha: 0.5488\ndefintely: 0.5488\nwoudl: 0.5487\nactuallity: 0.5486\nstupid.: 0.5486\nexpecialy: 0.5486\nworrie: 0.5485\n\n----------------------------------------\n\nAnalyzing word: serve\n\nTop 10 similar words (custom model):\nserved: 0.6769\nserver: 0.6466\nserving: 0.4830\nself: 0.4787\nservices: 0.4641\nstick: 0.4343\ntried: 0.4234\nvalet: 0.4041\ntogether: 0.3895\nasked: 0.3894\n\nTop 10 opposite words (custom model):\nliked: 0.0493\nsize: 0.0452\nlocal: 0.0435\ngrilled: 0.0425\nface: 0.0345\nthis: 0.0254\ngetting: 0.0034\ndelicious: -0.0041\ncheap: -0.0368\nlarge: -0.0458\n\nTop 10 similar words (pre-trained model):\nserves: 0.7344\nserving: 0.7307\nserved: 0.7221\ntoserve: 0.5691\nserve.The: 0.5484\nprovide: 0.5407\nServe: 0.5393\nserve.: 0.5372\nserve.We: 0.5273\npurpose--to: 0.5263\n\nTop 10 opposite words (pre-trained model):\nsubsume: 0.2844\nbeckon: 0.2843\nseek-out: 0.2843\nnot: 0.2843\norganize: 0.2842\nrefer: 0.2842\ndivinely-appointed: 0.2842\nbless: 0.2842\nservice.Please: 0.2841\nhumiliate: 0.2841\n\n----------------------------------------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Writting the results in pdf","metadata":{}},{"cell_type":"code","source":"from reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet\nimport random\n\n# Create a PDF document\npdf = SimpleDocTemplate(\"word_analysis_results.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\n\n# Define a function to find top similar and dissimilar words\ndef find_top_n(word, word_list, model, n=10):\n    similarities = model.wv.most_similar(word, topn=n)\n    return similarities\n\n# Define a function to write results to the PDF\ndef write_to_pdf(pdf, content_list):\n    # Create paragraphs for each content item\n    content = [Paragraph(item, styles[\"Normal\"]) for item in content_list]\n    pdf.build(content)\n\n# List to store analysis results\nanalysis_results = []\n\n# Loop to process words\nfor word in random.sample(words, 20):  # Take 20 random words from 'words' list\n    analysis_results.append(f\"Analyzing word: {word}\\n\")  # Add the word being analyzed\n\n    # Custom Model\n    similar_custom = find_top_n(word, words, fast_Text_model)\n    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n\n    # Pretrained Model\n    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n\n    # Add analysis results to the list\n    analysis_results.append(\"Top 10 similar words (custom model):\")\n    for similar_word, similarity in similar_custom:\n        analysis_results.append(f\"{similar_word}: {similarity:.4f}\")\n\n    analysis_results.append(\"\\nTop 10 opposite words (custom model):\")\n    for opposite_word, similarity in dissimilar_custom:\n        analysis_results.append(f\"{opposite_word}: {similarity:.4f}\")\n\n    analysis_results.append(\"\\nTop 10 similar words (pre-trained model):\")\n    for similar_word, similarity in similar_pretrained:\n        analysis_results.append(f\"{similar_word}: {similarity:.4f}\")\n\n    analysis_results.append(\"\\nTop 10 opposite words (pre-trained model):\")\n    for opposite_word, similarity in dissimilar_pretrained:\n        analysis_results.append(f\"{opposite_word}: {similarity:.4f}\")\n\n    analysis_results.append(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:29.674926Z","iopub.execute_input":"2024-04-22T09:26:29.679533Z","iopub.status.idle":"2024-04-22T09:26:37.166938Z","shell.execute_reply.started":"2024-04-22T09:26:29.679450Z","shell.execute_reply":"2024-04-22T09:26:37.165144Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion of two models","metadata":{}},{"cell_type":"code","source":"# Add conclusion\nconclusion = [\n    \"\\n\\nConclusion:\",\n    \"In this analysis, we explored the similarity and dissimilarity of words using both a custom FastText model and a pre-trained FastText model.\",\n    \"We found that at some words the pretrained model works better than the custom model as it really give very accurate results but at some other words the pretrained model just gives different forms of the same given word but the custom model gives diffrent words most of them are close to thegiven word meaning .\",\n    \"Overall, the results indicate that two models works pretty good but its word dependent if the word is rare or not in the pre-trained model's vocabulary, it may not perform well. \"\n]\n\n# Append conclusion to analysis_results\nanalysis_results += conclusion\n\n# Write the analysis results to the PDF\nwrite_to_pdf(pdf, analysis_results)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:37.169744Z","iopub.execute_input":"2024-04-22T09:26:37.171292Z","iopub.status.idle":"2024-04-22T09:26:37.560008Z","shell.execute_reply.started":"2024-04-22T09:26:37.171215Z","shell.execute_reply":"2024-04-22T09:26:37.558386Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Different way for writing in pdf using new different words\n","metadata":{}},{"cell_type":"code","source":"from reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\ndef write_to_pdf(pdf_file, analysis_results_custom, analysis_results_pretrained):\n    c = canvas.Canvas(pdf_file, pagesize=letter)\n    y = 750  # Starting y position\n    x_custom = 50  # Starting x position for custom model\n    x_pretrained = 350  # Starting x position for pretrained model\n    line_spacing = 15  # Spacing between lines\n    page_height = 800  # Height of the page\n    bottom_margin = 50  # Bottom margin\n\n    def check_new_page():\n        nonlocal y\n        if y < bottom_margin:\n            c.showPage()\n            y = page_height\n\n    for custom_line, pretrained_line in zip(analysis_results_custom, analysis_results_pretrained):\n        c.drawString(x_custom, y, custom_line)\n        c.drawString(x_pretrained, y, pretrained_line)\n        y -= line_spacing  # Adjust y position for next line\n        check_new_page()\n\n    c.save()\n\n# Usage\npdf_file = \"analysis_results.pdf\"\n# Define a function to find top similar and dissimilar words\ndef find_top_n(word, word_list, model, n=10):\n    similarities = model.wv.most_similar(word, topn=n)\n    return similarities\n\n\n# List to store analysis results\nanalysis_results_custom = []\nanalysis_results_pretrained = []\n\n# Loop to process words\nfor word in random.sample(words, 20):  # Take 20 random words from 'words' list\n    analysis_results_custom.append(f\"Analyzing word: {word}\\n\")  # Add the word being analyzed\n    analysis_results_pretrained.append(f\"Analyzing word: {word}\\n\")\n\n    # Custom Model\n    similar_custom = find_top_n(word, words, fast_Text_model)\n    dissimilar_custom = find_top_n(word, words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n\n    # Pretrained Model\n    similar_pretrained = find_top_n(word, words, pretrained_fastText_en)\n    dissimilar_pretrained = find_top_n(word, words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n\n    # Add analysis results to the list\n    analysis_results_custom.append(\"Top 10 similar words (custom model):\")\n    for similar_word, similarity in similar_custom:\n        analysis_results_custom.append(f\"{similar_word}: {similarity:.4f}\")\n\n    analysis_results_custom.append(\"\\nTop 10 opposite words (custom model):\")\n    for opposite_word, similarity in dissimilar_custom:\n        analysis_results_custom.append(f\"{opposite_word}: {similarity:.4f}\")\n\n    analysis_results_pretrained.append(\"\\nTop 10 similar words (pre-trained model):\")\n    for similar_word, similarity in similar_pretrained:\n        analysis_results_pretrained.append(f\"{similar_word}: {similarity:.4f}\")\n\n    analysis_results_pretrained.append(\"\\nTop 10 opposite words (pre-trained model):\")\n    for opposite_word, similarity in dissimilar_pretrained:\n        analysis_results_pretrained.append(f\"{opposite_word}: {similarity:.4f}\")\n\n    analysis_results_pretrained.append(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n    analysis_results_custom.append(\"\\n\" + \"-\"*40 + \"\\n\")\n\nwrite_to_pdf(pdf_file, analysis_results_custom, analysis_results_pretrained)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:26:37.561923Z","iopub.execute_input":"2024-04-22T09:26:37.562455Z","iopub.status.idle":"2024-04-22T09:26:45.681489Z","shell.execute_reply.started":"2024-04-22T09:26:37.562404Z","shell.execute_reply":"2024-04-22T09:26:45.680152Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# trying unseen random words ","metadata":{}},{"cell_type":"code","source":"import random\n\n\ndef find_top_n(word, word_list, model, n=10):\n    similarities = model.wv.most_similar(word, topn=n)\n    return similarities\n\ntest_words = [\"hi\", \"university\", \"college\", \"prepare\",\"convert\",\"process\"]\n\n# Loop to process words\nfor word in test_words:  # Take unseen words from test_words\n    print(f\"Analyzing word: {word}\\n\")  # Print the word being analyzed\n\n    # Custom Model\n    similar_custom = find_top_n(word, test_words, fast_Text_model)\n    dissimilar_custom = find_top_n(word, test_words, fast_Text_model, n=1000)[-10:]  # Get last 10 dissimilar\n    \n    # Pretrained Model\n    similar_pretrained = find_top_n(word, test_words, pretrained_fastText_en)\n    dissimilar_pretrained = find_top_n(word, test_words, pretrained_fastText_en, n=1000)[-10:]  # Get last 10 dissimilar\n    \n    # Print the output\n    print(\"Top 10 similar words (custom model):\")\n    for similar_word, similarity in similar_custom:\n        print(f\"{similar_word}: {similarity:.4f}\")\n    \n    print(\"\\nTop 10 opposite words (custom model):\")\n    for opposite_word, similarity in dissimilar_custom:\n        print(f\"{opposite_word}: {similarity:.4f}\")\n    \n    print(\"\\nTop 10 similar words (pre-trained model):\")\n    for similar_word, similarity in similar_pretrained:\n        print(f\"{similar_word}: {similarity:.4f}\")\n    \n    print(\"\\nTop 10 opposite words (pre-trained model):\")\n    for opposite_word, similarity in dissimilar_pretrained:\n        print(f\"{opposite_word}: {similarity:.4f}\")\n    \n    print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:31:35.428805Z","iopub.execute_input":"2024-04-22T09:31:35.429582Z","iopub.status.idle":"2024-04-22T09:31:38.093015Z","shell.execute_reply.started":"2024-04-22T09:31:35.429534Z","shell.execute_reply":"2024-04-22T09:31:38.091431Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Analyzing word: hi\n\nTop 10 similar words (custom model):\nhighly: 0.7362\nsushi: 0.7321\nhigh: 0.7231\nrecommended: 0.4403\nsplit: 0.4176\nbeware: 0.4139\nwebsite: 0.3999\nrecommend: 0.3968\npriced: 0.3934\nthursday: 0.3799\n\nTop 10 opposite words (custom model):\nbuilding: -0.0106\nmoney: -0.0162\nmost: -0.0195\nsign: -0.0256\nlocated: -0.0295\npickup: -0.0307\nstout: -0.0315\nwithout: -0.0329\nmove: -0.0422\nnext: -0.0438\n\nTop 10 similar words (pre-trained model):\nhi.: 0.7208\nhello: 0.7038\nHi: 0.6973\nhello.: 0.6432\nhi-: 0.6391\nhiii: 0.6334\nhiiiii: 0.6148\nHi.: 0.6143\nHi-: 0.6103\nhellow: 0.6086\n\nTop 10 opposite words (pre-trained model):\niwr: 0.4103\nuuh: 0.4102\niiy: 0.4102\njL: 0.4101\nloL: 0.4101\nwh: 0.4100\nmso: 0.4100\nidont: 0.4100\namanda.: 0.4099\nheI: 0.4099\n\n----------------------------------------\n\nAnalyzing word: university\n\nTop 10 similar words (custom model):\ncity: 0.6591\nquality: 0.6130\nunfortunately: 0.5982\nseemed: 0.5834\nvariety: 0.5806\ntrust: 0.5771\nunique: 0.5666\nwanted: 0.5605\nasian: 0.5599\ntrue: 0.5552\n\nTop 10 opposite words (custom model):\nlast: 0.2205\ndoor: 0.2149\nwhen: 0.2114\nawesome: 0.2114\nbeef: 0.2113\ncheck: 0.2102\ncoffee: 0.2100\nclose: 0.2088\ncold: 0.1996\nleast: 0.1877\n\nTop 10 similar words (pre-trained model):\nuniveristy: 0.7531\ncollege: 0.7493\nunivesity: 0.7314\nuniversities: 0.7240\nuniverstiy: 0.7224\nunversity: 0.7033\nuniverity: 0.6884\ncampus: 0.6862\nuniverisity: 0.6755\nuniversity.The: 0.6607\n\nTop 10 opposite words (pre-trained model):\nUniversity.A: 0.4032\nOUSU: 0.4031\npolytechnique: 0.4031\nschool-system: 0.4031\nShanghaiTech: 0.4031\nteacher-preparation: 0.4030\ngraduates.: 0.4030\nUPNG: 0.4030\ncollege-sponsored: 0.4029\nNTU: 0.4029\n\n----------------------------------------\n\nAnalyzing word: college\n\nTop 10 similar words (custom model):\nfootball: 0.5438\ndancing: 0.5361\ncrowd: 0.5199\ncenter: 0.5090\ncost: 0.5067\ncozy: 0.4985\nirish: 0.4957\nvenue: 0.4818\nbeignets: 0.4771\nholiday: 0.4760\n\nTop 10 opposite words (custom model):\nreviews: 0.0557\nmeal: 0.0536\nthings: 0.0535\nreally: 0.0497\nprices: 0.0439\ntold: 0.0418\nthem: 0.0415\ntasted: 0.0181\nordered: 0.0025\nvery: -0.0179\n\nTop 10 similar words (pre-trained model):\nuniversity: 0.7493\ncolleg: 0.6951\ngraduate: 0.6883\nschool: 0.6803\ncolllege: 0.6674\nhigh-school: 0.6658\ngrad: 0.6632\ncolleges: 0.6590\nstudent: 0.6506\ncolege: 0.6492\n\nTop 10 opposite words (pre-trained model):\ncollege-readiness: 0.3953\nteen: 0.3952\npost-football: 0.3952\nbatchmates: 0.3951\nPre-university: 0.3951\nWyoTech: 0.3951\nstudent-managed: 0.3951\nB.G.S.: 0.3951\nacads: 0.3950\npostbac: 0.3950\n\n----------------------------------------\n\nAnalyzing word: prepare\n\nTop 10 similar words (custom model):\nprepared: 0.9671\npossibly: 0.4700\nworst: 0.4650\nyourself: 0.4531\nraining: 0.4222\ntype: 0.3970\nwont: 0.3907\ntables: 0.3819\nquite: 0.3800\nlife: 0.3695\n\nTop 10 opposite words (custom model):\nyelp: 0.0251\nwell: 0.0203\nsign: 0.0126\nreally: 0.0121\nroast: 0.0064\ndark: 0.0062\nladies: 0.0047\nhoney: -0.0065\ngarlic: -0.0385\nopinion: -0.0438\n\nTop 10 similar words (pre-trained model):\npreparing: 0.7762\nprepared: 0.7180\nprepares: 0.6938\nPrepare: 0.6847\npreparation: 0.6575\npre-prepare: 0.6341\nperpare: 0.6306\nPreparing: 0.6064\nwell-prepared: 0.5771\nover-prepare: 0.5515\n\nTop 10 opposite words (pre-trained model):\nexplain: 0.3164\ngalvanize: 0.3163\nsneak: 0.3162\nwillneed: 0.3161\nstudying: 0.3160\nready.gov: 0.3160\ndemobilize: 0.3160\nsavour: 0.3159\nretake: 0.3159\nthis--to: 0.3159\n\n----------------------------------------\n\nAnalyzing word: convert\n\nTop 10 similar words (custom model):\ndessert: 0.6973\nconsistent: 0.6827\ncoconut: 0.6681\ncourtyard: 0.6130\ncozy: 0.6090\ncaramel: 0.6026\ncorn: 0.6003\ncook: 0.5885\nvelvet: 0.5818\ngotta: 0.5812\n\nTop 10 opposite words (custom model):\nthere: 0.1741\nwork: 0.1626\nwater: 0.1595\nwasnt: 0.1567\nwonderful: 0.1527\nphone: 0.1493\nthem: 0.1360\nthis: 0.1262\nthat: 0.1250\nwant: 0.1192\n\nTop 10 similar words (pre-trained model):\nreconvert: 0.7827\nconverts: 0.7810\nre-convert: 0.7757\nconverting: 0.7643\nconverted: 0.7360\nConvert: 0.7062\nconversion: 0.6642\nreconverting: 0.6280\nre-converted: 0.6268\nConverts: 0.6265\n\nTop 10 opposite words (pre-trained model):\nvCard: 0.2900\nintegrating: 0.2900\nparameterise: 0.2900\ninstalmentsYou: 0.2900\ncould: 0.2900\nChristianty: 0.2900\naccummulate: 0.2899\nfigure-out: 0.2899\nsynchronise: 0.2899\nre-sample: 0.2898\n\n----------------------------------------\n\nAnalyzing word: process\n\nTop 10 similar words (custom model):\nless: 0.6550\nguess: 0.6368\nunless: 0.6154\nbottomless: 0.5753\ncaramel: 0.5729\ngroceries: 0.5724\naddress: 0.5677\nproducts: 0.5588\ngotta: 0.5585\npete: 0.5472\n\nTop 10 opposite words (custom model):\nfriendly: 0.1937\ndont: 0.1916\nfull: 0.1898\nhere: 0.1813\nshow: 0.1707\nworth: 0.1684\ndinner: 0.1471\nlunch: 0.1313\nafter: 0.1223\nbusy: 0.1173\n\nTop 10 similar words (pre-trained model):\nprocess.This: 0.7102\nprocess.The: 0.7013\nprocesses: 0.6925\nproces: 0.6904\nprocess.When: 0.6807\nprocess--the: 0.6708\nprocess.It: 0.6701\nprocess.That: 0.6643\nprocess.But: 0.6387\nprocess.As: 0.6375\n\nTop 10 opposite words (pre-trained model):\nvote-counting: 0.3377\ndemystifies: 0.3377\nclinoid: 0.3376\nre-adjustment: 0.3376\ndeep-drawing: 0.3375\ncomplete.As: 0.3375\ncomplicating: 0.3374\ndocumentation: 0.3374\nre-tooling: 0.3373\nextrusion: 0.3373\n\n----------------------------------------\n\n","output_type":"stream"}]}]}